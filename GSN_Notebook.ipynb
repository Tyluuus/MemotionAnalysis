{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSN_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e3d05299db943f389f393c25fa67d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f7249241117c444980cc62c398ad82f7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c2fa10dbce8742da8a6ff9554aa5a0c3",
              "IPY_MODEL_f9f046cff5df4922a07de12e9a68da5e",
              "IPY_MODEL_3d3aea0f348f46ee8e5cc018a07713fb"
            ]
          }
        },
        "f7249241117c444980cc62c398ad82f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2fa10dbce8742da8a6ff9554aa5a0c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b8b22230a85541a291c3f1dcce442de4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4da8d21cfcee43d186f82f1f48508ee2"
          }
        },
        "f9f046cff5df4922a07de12e9a68da5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bf8828a4189648f6bc0847d0eb99a327",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a8a62216cc440b3b79ee91463e0fbdc"
          }
        },
        "3d3aea0f348f46ee8e5cc018a07713fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d8e9af03f65498fa5b82847b0fb8f90",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 15.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed6c801cc74e41dfb383bb44f649e96e"
          }
        },
        "b8b22230a85541a291c3f1dcce442de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4da8d21cfcee43d186f82f1f48508ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf8828a4189648f6bc0847d0eb99a327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a8a62216cc440b3b79ee91463e0fbdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d8e9af03f65498fa5b82847b0fb8f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed6c801cc74e41dfb383bb44f649e96e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3e4096c9f444062b5842adc2519e06f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a01a1303e235422a82bf07f774e3cf69",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bf8f461fa14d436e9da06cb05759651d",
              "IPY_MODEL_c62279550ce8475fb56ff3e10753b43c",
              "IPY_MODEL_8407e2d7ce3e46669169c49a2a5dd875"
            ]
          }
        },
        "a01a1303e235422a82bf07f774e3cf69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf8f461fa14d436e9da06cb05759651d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d1f31ac061944e3aa9eb5313bde57b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading:  27%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c41b283ccb754e49a165de1bbba7f4f9"
          }
        },
        "c62279550ce8475fb56ff3e10753b43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_413d4eeb5af342b4adcf16013f5086bc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 116152320,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_75f5f97884584023af31d0d83b322c70"
          }
        },
        "8407e2d7ce3e46669169c49a2a5dd875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a15e422049c146ba8111c7bd54d69458",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 111M/416M [00:54&lt;00:07, 43.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9870a4e07cea41b380bc3678507a6444"
          }
        },
        "8d1f31ac061944e3aa9eb5313bde57b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c41b283ccb754e49a165de1bbba7f4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "413d4eeb5af342b4adcf16013f5086bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "75f5f97884584023af31d0d83b322c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a15e422049c146ba8111c7bd54d69458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9870a4e07cea41b380bc3678507a6444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tyluuus/MemotionAnalysis/blob/udpates/GSN_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSN21Z Projekt "
      ],
      "metadata": {
        "id": "1rBvyUIKhIm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celem projektu będzie realizacja tasku B znajdującego się pod challengem Memotion na platformie Kaggle: https://www.kaggle.com/williamscott701/memotion-dataset-7k\n",
        "\n",
        "\n",
        "Treść zadania: *Task B- Humor Classification: Given an Internet meme, the system has to identify the type of humor expressed. The categories are sarcastic, humorous, and offensive meme. If a meme does not fall under any of these categories, then it is marked as another meme. A meme can have more than one category.*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HzZuFchUhK59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: W datasecie znajduje się 6992 memów pobranych z platformy reddit oraz oznaczonych z wykorzystaniem usługi Amazon Mechanical Turk. Oznaczenia znajdują się w pliku csv, który zawiera: \n",
        "1.   Nazwę pliku z memem\n",
        "2.   Tekst uzyskany z wykorzystaniem OCR\n",
        "3.   Tekst poprawiony\n",
        "4.   Klasa humorystyczna\n",
        "5.   Klasa sarkastyczna\n",
        "6.   Klasa ofensywna\n",
        "7.   Klasa motywacyjna\n"
      ],
      "metadata": {
        "id": "E0WtWWldhMuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie do użycia kodu"
      ],
      "metadata": {
        "id": "Erq9o3G_eKH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podłączenie notatnika do dysku Google - potrzebne przy zapisie checkpoint'ów i słowników stanów "
      ],
      "metadata": {
        "id": "lGx8n6QjeUnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "mXDbxxTh3ydL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sklonowanie repozytorium z githuba - pobranie plików składających się na dataset - zdjęć oraz pliku csv, pobierane są również pliki niezbędne do wczytania modelu sieci"
      ],
      "metadata": {
        "id": "ANEOn1zMenoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Tyluuus/MemotionAnalysis.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo0BnasUhoIN",
        "outputId": "d36d411e-ffef-4870-ca29-3184a3ad1da3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MemotionAnalysis' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalacja pakietu `transformers` użytym przy klasyfikatorze BERT"
      ],
      "metadata": {
        "id": "_Q1Kc-vxe0Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDlvwtuTQCHi",
        "outputId": "64e88350-876e-4ee1-ea65-f4fa62b2af01"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importy wszystkich bibliotek wykorzystywanych w projekcie"
      ],
      "metadata": {
        "id": "YgxEv063e8OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchtext.legacy.data import Dataset, Example, Field\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "4aknJXfFqGaI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przełączenie na używanie GPU w projekcie jeżeli jest to możliwe"
      ],
      "metadata": {
        "id": "rZsA7tHHfamw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to using GPU\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print(\"Using CUDA\")"
      ],
      "metadata": {
        "id": "hEbiFkj3tuUC",
        "outputId": "e6e724db-5a44-48da-f059-3d9ede2ed819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Określenie:\n",
        "- czy używany jest wytrenowany model, \n",
        "- czy wytrenowane modele powinny być zapisane,\n",
        "- czy wykonany ma być _tranfer learning_,\n",
        "- ścieżek do zapisu i odczytu modeli"
      ],
      "metadata": {
        "id": "ft6F-9Qyfh9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use trained model variable\n",
        "use_trained_model = False\n",
        "save_model = True\n",
        "image_model_continue_training = False\n",
        "image_save_model_path = './drive/MyDrive/GSN_dataset/memotion_images_model_5_epoch_vgg19.pt'\n",
        "text_save_model_path = './drive/MyDrive/GSN_dataset/memotion_text_model_bert_10_epochs.pt'\n",
        "image_load_model_path = './MemotionAnalysis/memotion_images_model_1_epoch.pt'\n",
        "text_load_model_path = './MemotionAnalysis/memotion_text_model_1_epoch.pt'\n",
        "image_checkpoint_path = './drive/MyDrive/GSN_dataset/checkpoint.pt'\n",
        "text_checkpoint_path = './drive/MyDrive/GSN_dataset/memotion_text_model_bert_10_epochs_checkpoint.pt'"
      ],
      "metadata": {
        "id": "B6v2cyDH1dKR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Określenie liczby epok dla trenowania gałęzi tekstu i obrazu oraz rozmiaru batchy - _same_batch_size_ jest wielkością dla jednoczesnego trenowania obrazu i testu z tą samą wielkością batcha"
      ],
      "metadata": {
        "id": "ube5qF9Fh-Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_epochs = 200 # 250\n",
        "image_epochs = 5 # 20\n",
        "text_batch_size = 2\n",
        "image_batch_size = 16\n",
        "\n",
        "same_batch_size = 32"
      ],
      "metadata": {
        "id": "XYO4NllQebSK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pobranie pretrenowanego modelu dla tekstu w razie potrzeby"
      ],
      "metadata": {
        "id": "lnHS0maKjChW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download text model for 300 epochs\n",
        "!gdown --id 1-9UMs4-3DO0acalwBUMf7rKHXBg1qV8n"
      ],
      "metadata": {
        "id": "6-kPaMfNz6Q0",
        "outputId": "90f17867-3b9f-49fb-edc7-d56e882c658a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-9UMs4-3DO0acalwBUMf7rKHXBg1qV8n\n",
            "To: /content/memotion_text_model_300_epoch.pt\n",
            "100% 5.49M/5.49M [00:00<00:00, 20.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Domyślnie trenowanymi modelami są VGG16 oraz LSTM. Wybór innych modeli możliwy jest w sekcjach odpowiedzialnych za poszczególne gałęzie."
      ],
      "metadata": {
        "id": "yHInTc-vN8BF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gałąź przetwarzania obrazu"
      ],
      "metadata": {
        "id": "b7Yd9DTVXfvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utworzenie zbioru danych na podstawie danych z Kaggle"
      ],
      "metadata": {
        "id": "6953TJgPGwop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie danych na zaimplementowanym obiekcie Dataset"
      ],
      "metadata": {
        "id": "h86FE3jYo3d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset designed to load data for Memotion Analisys Task from correspondig\n",
        "        Kaggle and assign weights for classes from task csv file\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path, low_data_mode=False, debug=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          csv_path (string): path to csv file with data \n",
        "          low_data_mode (boolean): low data mode for testing \n",
        "          debug (boolean): enable debug options\n",
        "        \"\"\"\n",
        "        # If system is in debug mode\n",
        "        self.debug = debug\n",
        "\n",
        "\n",
        "        # Read the csv_file\n",
        "        if low_data_mode==True:\n",
        "          self.data_info = pd.read_csv(csv_path, header = 6952)\n",
        "        else:\n",
        "          self.data_info = pd.read_csv(csv_path, header = 3)\n",
        "\n",
        "        # Column containing image names\n",
        "        self.image_arr = np.asarray(self.data_info.iloc[:, 1])\n",
        "\n",
        "        # Columns containing emotions classification\n",
        "        self.humour_arr = np.asarray(self.data_info.iloc[:, 4])\n",
        "        self.sarcasm_arr = np.asarray(self.data_info.iloc[:, 5])\n",
        "        self.offensive_arr = np.asarray(self.data_info.iloc[:, 6])\n",
        "        self.motivational_arr = np.asarray(self.data_info.iloc[:, 7])\n",
        "        \n",
        "\n",
        "\n",
        "        # Transforms performed on loaded image\n",
        "        self.data_transforms = transforms.Compose([\n",
        "                                      transforms.Resize((224, 224)),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        \n",
        "        # Array with class vectors for each image\n",
        "        self.labels = []\n",
        "\n",
        "        # Mapping word classification to 4 numeric classes\n",
        "        for index in range(len(self.humour_arr)):\n",
        "          humour_value = class_humour_weights[self.humour_arr[index]]\n",
        "          sarcasm_value = class_sarcasm_weights[self.sarcasm_arr[index]]\n",
        "          offensive_value = class_offensive_weights[self.offensive_arr[index]]\n",
        "          motivational_value = class_motivational_weights[self.motivational_arr[index]]\n",
        "\n",
        "          if humour_value > sarcasm_value:\n",
        "            if humour_value > offensive_value:\n",
        "              if humour_value > motivational_value:\n",
        "                var = 0\n",
        "              else:\n",
        "                var = 3 \n",
        "            else:\n",
        "              if offensive_value > motivational_value:\n",
        "                var = 2\n",
        "              else: \n",
        "                var = 3\n",
        "          else:\n",
        "            if sarcasm_value > offensive_value:\n",
        "              if sarcasm_value > motivational_value:\n",
        "                var = 1\n",
        "              else:\n",
        "                var = 3\n",
        "            else: \n",
        "              if offensive_value > motivational_value: \n",
        "                var = 2\n",
        "              else:\n",
        "                var = 3\n",
        "\n",
        "          # Creating class vector\n",
        "          lab = [0.0, 0.0, 0.0, 0.0]\n",
        "          lab[var] = 1.0\n",
        "          \n",
        "          # Adding new image class vector to labels array\n",
        "          self.labels.append(lab) \n",
        "\n",
        "        # Calculate of dataset\n",
        "        self.data_len = len(self.data_info.index)\n",
        "        \n",
        "        # Set correct path to images\n",
        "        self.image_arr = images_dir + self.image_arr\n",
        "\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          index (int): index of item to get  \n",
        "\n",
        "        Returns:\n",
        "          Tuple of image, text and class vector as tensors\n",
        "        \"\"\"\n",
        "        img_as_img = None\n",
        "        single_image_name = None\n",
        "\n",
        "\n",
        "        try:\n",
        "          # Get image name from pandas df\n",
        "          single_image_name = self.image_arr[index]\n",
        "\n",
        "          # # Open image with PIL and convert to RGB image\n",
        "          img = Image.open(single_image_name).convert('RGB')\n",
        "          if self.debug==True:\n",
        "            print('1:', img)\n",
        "\n",
        "          # Transform image and convert to tensor\n",
        "          img_as_tensor = self.data_transforms(img)\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('2:', img_as_tensor)\n",
        "\n",
        "          # Get class vector of the image from labels array\n",
        "          img_label = self.labels[index]\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('3:',img_label)\n",
        "\n",
        "          # Convert class vector to tensor\n",
        "          img_label = torch.as_tensor(img_label)\n",
        "          \n",
        "          if self.debug==True:\n",
        "            print('4:',img_label)\n",
        "\n",
        "          return (img_as_tensor, img_label)\n",
        "\n",
        "        except:\n",
        "          print(\"Image loading error for:\",single_image_name)\n",
        "          return ('ERROR', torch.tensor([-1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "metadata": {
        "id": "qAerCA5Gqd_D"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapowanie opisów z pliku csv na liczby zgodnie z wagą jaką reprezentują dla danego odczucia"
      ],
      "metadata": {
        "id": "dr0z-b0ApZTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionaries for mapping word classification\n",
        "class_humour_weights = {\"hilarious\": 3, \"not_funny\": 0, \"very_funny\": 2, \"funny\": 1}\n",
        "class_sarcasm_weights = {\"general\": 1, \"not_sarcastic\": 0, \"twisted_meaning\": 2, \"very_twisted\": 3}\n",
        "class_offensive_weights = {\"not_offensive\": 0, \"slight\": 1, \"very_offensive\": 2, \"hateful_offensive\": 3}\n",
        "class_motivational_weights = {\"not_motivational\": 0, \"motivational\": 1}\n",
        "\n",
        "# Directory containing images\n",
        "images_dir = \"./MemotionAnalysis/images/\""
      ],
      "metadata": {
        "id": "v6vAEOVpsjht"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie danych oraz utworzenie z nich dataloader'a"
      ],
      "metadata": {
        "id": "6V9Ac1BYrqQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading images into custom dataset\n",
        "dataset = MyCustomDataset('MemotionAnalysis/labels.csv', low_data_mode=False)\n",
        "\n",
        "# Loading dataset into DataLoader and setting batch_size\n",
        "b_size = same_batch_size if use_trained_model else image_batch_size\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=b_size, shuffle=False, num_workers=1)\n",
        "dataset_size = len(dataloader)"
      ],
      "metadata": {
        "id": "QyONsUPTtxnL"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdzenie wczytywanych danych"
      ],
      "metadata": {
        "id": "7QvdybtDt0MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check loaded data\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "def show_databatch(inputs, classes):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    imshow(out, title=[classes])\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloader))\n",
        "show_databatch(inputs, classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "XQ6mkihFuZ0B",
        "outputId": "bec67174-4e92-4314-d93c-def6708d5b08"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definicja modelu i uczenie"
      ],
      "metadata": {
        "id": "NtI-EnR9hPkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wybór modelu obrazu z dwóch przetestowanych modeli VGG16 i VGG19"
      ],
      "metadata": {
        "id": "9_13Es0Xv1uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# avaliable models: vgg_16, vgg_19\n",
        "image_model_type = 'vgg_16' # 'vgg_19'"
      ],
      "metadata": {
        "id": "Orvtw4N67SIt"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pobranie słownika stanu dla pretrenowanego VGG 16"
      ],
      "metadata": {
        "id": "RtjesP7Dv8pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading pretrained vgg16\n",
        "\n",
        "!gdown --id 1f3iqJY2NDTsJP2_BFmtk5WtcfAW9nwdl"
      ],
      "metadata": {
        "id": "9lJ81u6u4Em2",
        "outputId": "65e39965-64d1-441a-c16f-c1ece97f3261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f3iqJY2NDTsJP2_BFmtk5WtcfAW9nwdl\n",
            "To: /content/vgg16_bn.pth\n",
            "100% 554M/554M [00:07<00:00, 72.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zainicjowanie modelu VGG w wybranej wersji"
      ],
      "metadata": {
        "id": "Cz-BoDtdwHA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = None\n",
        "\n",
        "if image_model_type == 'vgg_16':\n",
        "  vgg = models.vgg16_bn()\n",
        "  vgg.load_state_dict(torch.load(\"/content/vgg16_bn.pth\")) \n",
        "  print(\"Used model: VGG16\")\n",
        "elif image_model_type == 'vgg_19':\n",
        "  vgg = models.vgg19_bn()\n",
        "  print(\"Used model: VGG19\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiM8HLi7umZR",
        "outputId": "f65f37e8-171d-48f4-a118-af0843ba94c0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used model: VGG16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Określenie klasy _custom_vgg_, która pozwala odtworzyć architekturę baseline ze \n",
        "źródła: w porównaniu z bazowym VGG zmieniony jest klasyfikator, po wyciągnięciu\n",
        "feature'ów przeprowadzany jest pooling, nastepnie wynik przekazywany jest do warstwy fully connected i poddawany funkcji sigmoid - wynikiem jest wektor czteroelementowy."
      ],
      "metadata": {
        "id": "ZhYCy1ApwPgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if use_trained_model==False:\n",
        "\n",
        "  # Class with own modification of VGG16 architecture - classifier was changed - \n",
        "  # after getting features from image there is pooling layer, next results are flattened and feed to fully connected layer with output number = 4\n",
        "  # at the end sigmoid function is used\n",
        "\n",
        "class custom_vgg(nn.Module):\n",
        "    \"\"\"\n",
        "        Custom vgg implementation as described in baseline in in Memotion Analisys \n",
        "        task summary (https://arxiv.org/pdf/2008.03781.pdf)\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                model: base VGG model downloaded from source\n",
        "        \"\"\"\n",
        "        super(custom_vgg, self).__init__()\n",
        "\n",
        "        self.features = list(model.features)\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "        self.pooling = model.avgpool\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(in_features=25088, out_features=4, bias=True)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                x: object to perform learning on\n",
        "        \"\"\"\n",
        "        out = self.features(x)\n",
        "        out = self.pooling(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigm(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "7-AWBscoun7-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjalizacja modelu _custom_vgg_ na wgranym wcześniej modelu VGG"
      ],
      "metadata": {
        "id": "tbQHvVYC68Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading custiom VGG16 from loaded, pretrained VGG16 model\n",
        "vgg = custom_vgg(vgg)\n",
        "\n",
        "# if use_trained_model==True:\n",
        "#   vgg16.load_state_dict(torch.load('/content/drive/MyDrive/GSN_dataset/memotion_images_model.pt'))"
      ],
      "metadata": {
        "id": "RcNr3Wd6uph2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przełączenie na korzystanie z GPU jeżeli to możliwe oraz określenie krytrium, optimizera i learning rate schedulera dla pętli uczenia"
      ],
      "metadata": {
        "id": "JcMhFeEj7FSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set VGG to run on GPU\n",
        "if use_gpu:\n",
        "    vgg.cuda()\n",
        "    \n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define optimizer and LR Scheduler for training\n",
        "optimizer_ft = optim.SGD(vgg.parameters(), lr=0.001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "IW9Ss-wxuu90"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja słownika danych dla obliczania makro F1 wraz z funkcjami operującymi na słowniku"
      ],
      "metadata": {
        "id": "0GObuiBT7S9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 score calculating for image\n",
        "image_f1_data = {\n",
        "    0: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    1: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    2: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    3: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    }\n",
        "}\n",
        "\n",
        "def clear_image_f1_data():\n",
        "    \"\"\"\n",
        "        Clearnig realted F1 data dict\n",
        "    \"\"\"\n",
        "    for i in range(4):\n",
        "        image_f1_data[i][\"true_positive\"] = 0.0\n",
        "        image_f1_data[i][\"false_positive\"] = 0.0\n",
        "        image_f1_data[i][\"false_negative\"] = 0.0\n",
        "\n",
        "def update_image_f1_data(preditions: list, labels: list):\n",
        "    \"\"\"\n",
        "        Updating F1 data dict with list of predicted class and actual labels\n",
        "        Args:\n",
        "            predictions (list): list of predicted classes with data in range 0-3\n",
        "            labels (list): list of actual classes with data in range 0-3\n",
        "    \"\"\"\n",
        "    for i in range(len(preditions)):\n",
        "        if(preditions[i] == labels[i]):\n",
        "            image_f1_data[labels[i]][\"true_positive\"] += 1.0\n",
        "        else:\n",
        "            image_f1_data[labels[i]][\"false_positive\"] += 1.0\n",
        "            image_f1_data[preditions[i]][\"false_negative\"] += 1.0\n",
        "                \n",
        "\n",
        "def calculate_image_f1_for_class(class_number: int):\n",
        "    \"\"\"\n",
        "        Calculating F1 for given class number based on data in F1 data dict\n",
        "        Args:\n",
        "            class_number(int): Class number of which F1 will be calculated\n",
        "    \"\"\"\n",
        "    precision_divider = image_f1_data[class_number][\"true_positive\"]+image_f1_data[class_number][\"false_positive\"]\n",
        "    precision = (image_f1_data[class_number][\"true_positive\"] / precision_divider) if precision_divider > 0 else 0\n",
        "    recall_divider = image_f1_data[class_number][\"true_positive\"]+image_f1_data[class_number][\"false_negative\"]\n",
        "    recall = (image_f1_data[class_number][\"true_positive\"] / recall_divider) if recall_divider > 0 else 0\n",
        "    #print(f\"Precision: {precision} , Recall: {recall} \")\n",
        "    return (2 * (precision * recall) / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "def calculate_image_macro_f1():\n",
        "    \"\"\"\n",
        "        Calculates macro F1 based on F1 score of each class - based on `calculate_image_f1_for_class`\n",
        "        function\n",
        "    \"\"\"\n",
        "    macro_f1 = 0.0\n",
        "    for i in range(4):\n",
        "        macro_f1 += calculate_image_f1_for_class(i)\n",
        "    return macro_f1/4"
      ],
      "metadata": {
        "id": "kU4rv8uUXgPw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja funkcji pomocniczej konwertującej klasy jako indeksy w macierzy do listy z indeksami na których występują dane, przykład:\n",
        "```\n",
        "[[1, 0, 0, 0],\n",
        " [0, 0, 1, 0],\n",
        " [0, 1, 0, 0],  \n",
        " [0, 0, 0, 1]]  = [0, 2, 1 ,3]\n",
        "```\n",
        "Funckja potrzebna jest przy uczeniu"
      ],
      "metadata": {
        "id": "uZcXB0Dc832l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classes_convert(classes):\n",
        "    \"\"\"\n",
        "        Converts matrix of indices (classes) to list with indices of value ex:\n",
        "        [[1, 0, 0, 0],\n",
        "         [0, 0, 1, 0],\n",
        "         [0, 1, 0, 0],  \n",
        "         [0, 0, 0, 1]]  = [0, 2, 1 ,3]\n",
        "        Args:\n",
        "            classes (list): list of lists for classes which will be converted\n",
        "    \"\"\"\n",
        "    clas = []\n",
        "    for element in range(len(classes)):\n",
        "        var = classes[element]\n",
        "        for index in range(len(var)):\n",
        "            if var[index]==1:\n",
        "                clas.append(index)\n",
        "    return torch.tensor(clas)"
      ],
      "metadata": {
        "id": "vEeAe-_azz_h"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funckja służąca do wczytania słownika stanów z pliku zewnętrznego, używana przy _transfer_learning_ oraz użyciu gotowych modeli"
      ],
      "metadata": {
        "id": "WoGPogCPBl-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, filename):\n",
        "    \"\"\"\n",
        "        Load pretrained data to model and optimizer from file\n",
        "        Args:\n",
        "            model (nn.Model): model object to which data will be loaded\n",
        "            optimizer (torch.optim): optimizer obcject to which data will be loaded\n",
        "            filename (string): file from which data will be loaded\n",
        "    \"\"\"\n",
        "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
        "    if os.path.isfile(filename):\n",
        "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "        checkpoint = torch.load(filename)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"=> loaded checkpoint '{}'\"\n",
        "                  .format(filename))\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "E4mcdJkWeiiR"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja funkcji uczącej dla gałęzi obrazu dla VGG"
      ],
      "metadata": {
        "id": "anis2HKgGlJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining training model\n",
        "def train_model(vgg, criterion, optimizer, scheduler, num_epochs=1, debug=False):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            vgg (nn.Model): Neural Network model to traing\n",
        "            criterion (nn.LossFunction): Loss Function \n",
        "            optimizer (torch.optim): Optimalization Function\n",
        "            scheduler (torch.optim.lr_scheduler): Learning Rate Scheduler\n",
        "            num_epochs (int): Number of training epochs\n",
        "            debug (boolean): Debug mode toogle\n",
        "    \"\"\"\n",
        "\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "    best_acc = 0.0\n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    avg_loss_val = 0\n",
        "    avg_acc_val = 0\n",
        "    \n",
        "    train_batches = len(dataloader)\n",
        "    val_batches = len(dataloader)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        loss_train = 0\n",
        "        loss_val = 0\n",
        "        acc_train = 0\n",
        "        acc_val = 0\n",
        "        target_true = 0\n",
        "        predicted_true = 0\n",
        "        correct_true = 0\n",
        "        \n",
        "        vgg.train(True)\n",
        "        clear_image_f1_data()\n",
        "        for inputs, classes in iter(dataloader):\n",
        "            if use_gpu:\n",
        "                sample, clas = Variable(inputs.cuda()), Variable(classes.cuda())\n",
        "            else:\n",
        "                sample, clas = Variable(inputs), Variable(classes)\n",
        "            \n",
        "            # addressing batch labels to list\n",
        "            batch_labels = []\n",
        "            for row in clas.data:\n",
        "              for i in range(len(row)):\n",
        "                if row[i] == 1:\n",
        "                  batch_labels.append(i)\n",
        "            \n",
        "            if use_gpu:\n",
        "              batch_labels = torch.tensor(batch_labels).cuda()\n",
        "            else:\n",
        "              batch_labels = torch.tensor(batch_labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = vgg(sample)\n",
        "\n",
        "            if debug==True:\n",
        "              print(outputs)\n",
        "              print(clas)\n",
        "\n",
        "            _, preds = torch.max(outputs.data, -1)\n",
        "            loss = criterion(outputs, clas)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            update_image_f1_data(preds.squeeze().tolist(), batch_labels.squeeze().tolist())\n",
        "            \n",
        "            del sample, clas, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        epoch_train_macro_f1 = calculate_image_macro_f1()\n",
        "        \n",
        "        vgg.train(False)\n",
        "        vgg.eval()\n",
        "        clear_image_f1_data()    \n",
        "        for inputs, classes in iter(dataloader):\n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda()), Variable(classes.cuda())\n",
        "            else:\n",
        "                inputs, labels = Variable(inputs), Variable(classes)\n",
        "\n",
        "            # addressing batch labels to list\n",
        "            batch_labels = []\n",
        "            for row in labels.data:\n",
        "              for i in range(len(row)):\n",
        "                if row[i] == 1:\n",
        "                  batch_labels.append(i)\n",
        "            \n",
        "            if use_gpu:\n",
        "              batch_labels = torch.tensor(batch_labels).cuda()\n",
        "            else:\n",
        "              batch_labels = torch.tensor(batch_labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = vgg(inputs)\n",
        "            \n",
        "            # Prediction\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            if debug==True:\n",
        "              print(\"Preds: \",preds)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if debug==True:\n",
        "              print(\"[1]Classes shape: \",classes.shape)\n",
        "              print(\"[1]Classes: \",classes)\n",
        "\n",
        "            classes = classes_convert(classes)\n",
        "            if debug==True:\n",
        "              print(\"[2]Classes: \",classes.shape)\n",
        "              print(\"[2]Classes: \",classes)\n",
        "            \n",
        "            update_image_f1_data(preds.squeeze().tolist(), batch_labels.squeeze().tolist())\n",
        "            del inputs, labels, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        epoch_val_macro_f1 = calculate_image_macro_f1()\n",
        "\n",
        "        print()\n",
        "        print(\"Epoch {} result: \".format(epoch))\n",
        "        print(\"F1 score: {:.4f} (train), {:.4f} (val)\".format(epoch_train_macro_f1, epoch_val_macro_f1))\n",
        "        print('-' * 10)\n",
        "        print()\n",
        "        \n",
        "        if avg_acc_val > best_acc:\n",
        "            best_acc = avg_acc_val\n",
        "            best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "        \n",
        "    elapsed_time = time.time() - since\n",
        "    print()\n",
        "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
        "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
        "    \n",
        "    vgg.load_state_dict(best_model_wts)\n",
        "    return vgg"
      ],
      "metadata": {
        "id": "d_glLctDu338"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uruchomienie uczenia w odpowiedniej wersji zgodnej z określonymi na początku założeniami tj. trenowanie od początku albo _transfer learning_ oraz zapisywanie modelu, gdy zostało to określone"
      ],
      "metadata": {
        "id": "ZMixVzzhLBAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_trained_model==False and image_model_continue_training == False:\n",
        "  vgg = train_model(vgg, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=image_epochs, debug=False)\n",
        "  if save_model:\n",
        "    torch.save(vgg.state_dict(), image_save_model_path)\n",
        "    print(\"Image model saved\")\n",
        "    checkpoint = {'state_dict': vgg.state_dict(), 'optimizer': optimizer_ft.state_dict()}\n",
        "    torch.save(checkpoint, image_checkpoint_path)\n",
        "    print(\"Checkpoint saved\")\n",
        "\n",
        "elif use_trained_model==False and image_model_continue_training == True:\n",
        "  \n",
        "  checkpoint_model, optimizer = load_checkpoint(vgg, optimizer_ft,image_checkpoint_path)\n",
        "  checkpoint_model = checkpoint_model.cuda()\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "  for state in optimizer.state.values():\n",
        "    for k,v in state.items():\n",
        "      if isinstance(v, torch.Tensor):\n",
        "        state[k] = v.cuda()\n",
        "        \n",
        "  vgg = train_model(checkpoint_model, criterion, optimizer, lr_scheduler, num_epochs=image_epochs, debug=False)\n",
        "  if save_model:\n",
        "    torch.save(vgg.state_dict(), image_save_model_path)\n",
        "    print(\"Image model saved\")\n",
        "    checkpoint = {'state_dict': vgg.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    torch.save(checkpoint, image_checkpoint_path)\n",
        "    print(\"Checkpoint saved\")\n",
        "    "
      ],
      "metadata": {
        "id": "8V6CUYAQu6A6",
        "outputId": "e3226379-2207-4f15-e82f-875260040563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 result: \n",
            "F1 score: 0.2217 (train), 0.2062 (val)\n",
            "----------\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image loading error for: ./MemotionAnalysis/images/image_6774.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-4367e6153ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_trained_model\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimage_model_continue_training\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_save_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image model saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-cad294923b1b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(vgg, criterion, optimizer, scheduler, num_epochs, debug)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mclear_image_f1_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gałąź przekształcania tekstu"
      ],
      "metadata": {
        "id": "63L0we3L_NCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dla przetwarzania tekstu możliwe jest użycie dwóch klasyfikatorów: (Bi)LSTM lub BERT"
      ],
      "metadata": {
        "id": "7e_LFAiuNB9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utworzenie zbioru danych dla tekstu na podstawie zbioru z Kaggle"
      ],
      "metadata": {
        "id": "U6g8zLdpXqGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utworzenie obiektów Field na podstawie tekstu wyciągnietego ze Dataset'u określanego w gałęzi obrazu. Tekst poddawany jest embeddingowi z użyciem GloVe 100d."
      ],
      "metadata": {
        "id": "ltoji6Y2QN88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "\n",
        "meme_text = dataset.data_info.iloc[:, 3]\n",
        "\n",
        "raw_df = []\n",
        "\n",
        "for i in range(len(meme_text)):\n",
        "    raw_df.append([str(meme_text[i]), dataset.labels[i]])\n",
        "\n",
        "df = pd.DataFrame(raw_df[:-3], columns=['text', 'label'])\n",
        "\n",
        "text_field = Field(\n",
        "    sequential=True,\n",
        "    tokenize='basic_english', \n",
        "    fix_length=64,\n",
        "    lower=True\n",
        ")\n",
        "label_field = Field(sequential=False, use_vocab=False)\n",
        "# prepocess\n",
        "preprocessed_text = df['text'].apply(\n",
        "    lambda x: text_field.preprocess(x)\n",
        ")\n",
        "# load fastext simple embedding with 100d\n",
        "text_field.build_vocab(\n",
        "    preprocessed_text, \n",
        "    vectors='glove.6B.100d'\n",
        ")"
      ],
      "metadata": {
        "id": "Rptz647rV-0L"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utworzenie zbiorów danych - uczącego i walidacyjnego - na podstawie wgranych DataFrame z biblioteki `pandas`"
      ],
      "metadata": {
        "id": "aRKErMI4SUx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataFrameDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset created drom pandas DataFrame with text data from Kaggle for \n",
        "        Memotion Analisys task\n",
        "    \"\"\"\n",
        "    def __init__(self, df: pd.DataFrame, fields: list):\n",
        "        super(DataFrameDataset, self).__init__(\n",
        "            [\n",
        "                Example.fromlist(list(r), fields) for i, r in df.iterrows()\n",
        "            ], \n",
        "            fields\n",
        "        )\n",
        "\n",
        "train_dataset, test_dataset = DataFrameDataset(\n",
        "    df=df, \n",
        "    fields=(\n",
        "        ('text', text_field),\n",
        "        ('label', label_field)\n",
        "    )\n",
        ").split(split_ratio=0.85)\n",
        "\n",
        "b_size = same_batch_size if use_trained_model else text_batch_size\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(\n",
        "    datasets=(train_dataset, test_dataset), \n",
        "    batch_sizes=(b_size, b_size),\n",
        "    sort=False\n",
        ")"
      ],
      "metadata": {
        "id": "P6qhrLcvWeyu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja słownika danych dla obliczania makro F1 wraz z funkcjami operującymi na słowniku"
      ],
      "metadata": {
        "id": "nIZWmSOFZbFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_f1_data = {\n",
        "    0: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    1: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    2: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    3: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    }\n",
        "}\n",
        "\n",
        "def clear_text_f1_data():\n",
        "    \"\"\"\n",
        "        Clearnig realted F1 data dict\n",
        "    \"\"\"\n",
        "    for i in range(4):\n",
        "        text_f1_data[i][\"true_positive\"] = 0.0\n",
        "        text_f1_data[i][\"false_positive\"] = 0.0\n",
        "        text_f1_data[i][\"false_negative\"] = 0.0\n",
        "\n",
        "def update_text_f1_data(preditions: list, labels: list):\n",
        "    \"\"\"\n",
        "        Updating F1 data dict with list of predicted class and actual labels\n",
        "        Args:\n",
        "            predictions (list): list of predicted classes with data in range 0-3\n",
        "            labels (list): list of actual classes with data in range 0-3\n",
        "    \"\"\"\n",
        "    if type(preditions) is int:\n",
        "      preditions = [preditions]\n",
        "    if type(labels) is int:\n",
        "      labels = [labels]\n",
        "    for i in range(min(len(preditions), len(labels))):\n",
        "        if(preditions[i] == labels[i]):\n",
        "            text_f1_data[labels[i]][\"true_positive\"] += 1.0\n",
        "        else:\n",
        "            text_f1_data[labels[i]][\"false_positive\"] += 1.0\n",
        "            text_f1_data[preditions[i]][\"false_negative\"] += 1.0\n",
        "                \n",
        "\n",
        "def calculate_text_f1_for_class(class_number: int):\n",
        "    \"\"\"\n",
        "        Calculating F1 for given class number based on data in F1 data dict\n",
        "        Args:\n",
        "            class_number(int): Class number of which F1 will be calculated\n",
        "    \"\"\"\n",
        "    if text_f1_data[class_number][\"true_positive\"] == 0:\n",
        "      return 0\n",
        "    precision = text_f1_data[class_number][\"true_positive\"] / (text_f1_data[class_number][\"true_positive\"]+text_f1_data[class_number][\"false_positive\"])\n",
        "    recall = text_f1_data[class_number][\"true_positive\"] / (text_f1_data[class_number][\"true_positive\"]+text_f1_data[class_number][\"false_negative\"])\n",
        "    # print(f\"Precision: {precision} , Recall: {recall} \")\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def calculate_text_macro_f1():\n",
        "    \"\"\"\n",
        "        Calculates macro F1 based on F1 score of each class - based on `calculate_image_f1_for_class`\n",
        "        function\n",
        "    \"\"\"\n",
        "    macro_f1 = 0.0\n",
        "    for i in range(4):\n",
        "      macro_f1 += calculate_text_f1_for_class(i)\n",
        "    return macro_f1/4"
      ],
      "metadata": {
        "id": "qgTTp_11Xc7i"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funkcja pomocnicza mająca na celu poprawne zapisanie klas poprzez znalezienie maksymalnego prawdopodobieństwa zwróconego przez klasyfikator. Używana przy uczeniu tekstu w klasyfikatorze LSTM."
      ],
      "metadata": {
        "id": "VYPGtVTRet7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_classes_convert(classes):\n",
        "    \"\"\"\n",
        "        Maps output max from probabilities to binary classes, ex.\n",
        "        [0.6, 0.4, 0.3, 0.3] = [1, 0, 0, 0]\n",
        "    \"\"\"\n",
        "    y = classes\n",
        "    # Actual conversion using y elements as index \n",
        "    M = np.zeros(len(y))\n",
        "\n",
        "    for i in range(len(y)):\n",
        "        M[i]=torch.argmax(y[i])\n",
        "    return torch.tensor(M)"
      ],
      "metadata": {
        "id": "8HoqPaoUYVj_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "RM1e5Ww0_cNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ze względu na odmienne podejście w klasyfikatorze BERT zbiór danych dla niego tworzony jest osobno, również na podstawie DataField z biblioteki `pandas`"
      ],
      "metadata": {
        "id": "veb9isFChjzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset for BERT classificator with text data from Kaggle for \n",
        "        Memotion Analisys task\n",
        "    \"\"\"\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "            df - pandas DataFrame object \n",
        "        \"\"\"\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "        self.labels = [np.argmax(l) for l in df['label']]\n",
        "        self.data = [self.tokenizer(t, \n",
        "                            padding=\"max_length\", max_length=128,\n",
        "                            truncation=True, return_tensors=\"pt\") for t in df['text']]\n",
        "\n",
        "    def classes(self):\n",
        "        \"\"\"\n",
        "            Get all the classes for classifier\n",
        "        \"\"\"\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "            Get all the classes len for classifier\n",
        "        \"\"\"\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        \"\"\"\n",
        "            Fetch a batch of labels\n",
        "        \"\"\"\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        \"\"\"\n",
        "            Fetch a batch of inputs\n",
        "        \"\"\"\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "            Fetch a batch of inputs and correspoding labels\n",
        "            Returns:\n",
        "                Tuple of batch of text and batch of correspoding labels\n",
        "        \"\"\"\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "TeX0OrYcc7tt"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjacja zbiorów testowego, walidacyjnego i uczącego, sprawdzenie długości zbiorów"
      ],
      "metadata": {
        "id": "6AOxVAnXkPrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
        "                                     [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "print(len(df_train),len(df_val), len(df_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey4v9TE_dczE",
        "outputId": "bd3831d3-107a-4db2-cf84-90479c4a3755"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5585 698 699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja klasyfikatora BERT"
      ],
      "metadata": {
        "id": "3EgZMd0elI_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "        BERT classifier baser on PyTorch BERT clasifier, using pretrained\n",
        "        data\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.5):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 4)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "        \"\"\"\n",
        "            Perform train on given input\n",
        "            Args:\n",
        "                input_id (int) - input id on which learning should be performed\n",
        "                mask - attention mask which will be used in training\n",
        "            Returns:\n",
        "                tensor of predicted classes\n",
        "        \"\"\"\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "1Ik7CuJfeIu8"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja funkcji uczenia i evaluacji modelu BERT"
      ],
      "metadata": {
        "id": "VZgas9Raql7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text model train - BERT\n",
        "\n",
        "def train_bert(model, train_data, val_data, lr, epochs):\n",
        "    \"\"\"\n",
        "        Training BERT model and saving if global config is set to\n",
        "        Args:\n",
        "            model (nn.Model) - BertModel to train\n",
        "            train_data (Dataset) - dataset with data to train\n",
        "            val_Set (Dataset) - dataset to evaluate\n",
        "            lr (float) - learning rate\n",
        "            epochs (int) - number of epochs\n",
        "    \"\"\"\n",
        "    train, val = BertDataset(train_data), BertDataset(val_data)\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr = lr)\n",
        "    \n",
        "    if use_cuda:\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "            clear_text_f1_data()\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "                batch_loss = criterion(output, train_label)\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "                update_text_f1_data(output.argmax(dim=1).tolist(), train_label.squeeze().tolist())\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "            epoch_train_macro_f1 = calculate_text_macro_f1()\n",
        "            clear_text_f1_data()\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label)\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    update_text_f1_data(output.argmax(dim=1).tolist(), val_label.squeeze().tolist())\n",
        "                    total_acc_val += acc\n",
        "            epoch_val_macro_f1 = calculate_text_macro_f1()\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f} \\\n",
        "                | Train macro F1: {epoch_train_macro_f1} \\\n",
        "                | Val macro F1: {epoch_val_macro_f1} \\\n",
        "            ')\n",
        "            if save_model:\n",
        "                torch.save(model.state_dict(), text_save_model_path)\n",
        "                print(\"Image model saved\")\n",
        "                checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "                torch.save(checkpoint, text_checkpoint_path)\n",
        "                print(\"Checkpoint saved\")\n",
        "\n",
        "def evaluate_bert(model, test_data):\n",
        "    \"\"\"\n",
        "        Evaluating BERT model on given set\n",
        "        Args:\n",
        "            model (nn.Model) - BERT model\n",
        "            test_dataset (Dataset) - dataset to evaluate BERT on\n",
        "    \"\"\"\n",
        "\n",
        "    test = BertDataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    clear_text_f1_data()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "\n",
        "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "              total_acc_test += acc\n",
        "              update_text_f1_data(output.argmax(dim=1).tolist(), test_label.squeeze().tolist())\n",
        "    \n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "    print(\"F1 score: {:.4f}\".format(calculate_text_macro_f1()))"
      ],
      "metadata": {
        "id": "xhc7s-Xid2Ta"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uruchomienie trenowania modelu BERT"
      ],
      "metadata": {
        "id": "SUFJH5PHsvdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BertClassifier()\n",
        "LR = 1e-6\n",
        "              \n",
        "train_bert(bert_model, df_train, df_val, LR, text_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444,
          "referenced_widgets": [
            "4e3d05299db943f389f393c25fa67d1e",
            "f7249241117c444980cc62c398ad82f7",
            "c2fa10dbce8742da8a6ff9554aa5a0c3",
            "f9f046cff5df4922a07de12e9a68da5e",
            "3d3aea0f348f46ee8e5cc018a07713fb",
            "b8b22230a85541a291c3f1dcce442de4",
            "4da8d21cfcee43d186f82f1f48508ee2",
            "bf8828a4189648f6bc0847d0eb99a327",
            "2a8a62216cc440b3b79ee91463e0fbdc",
            "4d8e9af03f65498fa5b82847b0fb8f90",
            "ed6c801cc74e41dfb383bb44f649e96e",
            "f3e4096c9f444062b5842adc2519e06f",
            "a01a1303e235422a82bf07f774e3cf69",
            "bf8f461fa14d436e9da06cb05759651d",
            "c62279550ce8475fb56ff3e10753b43c",
            "8407e2d7ce3e46669169c49a2a5dd875",
            "8d1f31ac061944e3aa9eb5313bde57b8",
            "c41b283ccb754e49a165de1bbba7f4f9",
            "413d4eeb5af342b4adcf16013f5086bc",
            "75f5f97884584023af31d0d83b322c70",
            "a15e422049c146ba8111c7bd54d69458",
            "9870a4e07cea41b380bc3678507a6444"
          ]
        },
        "id": "kZx8aIJBfMYQ",
        "outputId": "1b02928d-a8ce-41dd-888d-212fd9ab7c35"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e3d05299db943f389f393c25fa67d1e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3e4096c9f444062b5842adc2519e06f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-bb523f473bfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-667bb0f9a931>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dropout)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \"\"\"\n\u001b[1;32m     11\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1335\u001b[0m                     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m                     \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m                     \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m                 )\n\u001b[1;32m   1339\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1782\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m         )\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   2038\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m     )\n\u001b[0;32m-> 1899\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ewaluacja modelu BERT"
      ],
      "metadata": {
        "id": "z5Z9-qPo0yEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bert(bert_model, df_test)"
      ],
      "metadata": {
        "id": "dpEshY9vmvHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "5ycjOcf8_jMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Określenie modelu LSTM"
      ],
      "metadata": {
        "id": "fUpdCsvR01lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if use_trained_model==False:\n",
        "\n",
        "class ModelParam(object):\n",
        "    \"\"\"\n",
        "        Class with parameters for TextModel\n",
        "    \"\"\"\n",
        "    def __init__(self, param_dict: dict = dict()):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                param_dict - dict with data for TextModel, possible fields:\n",
        "                    - input_size (batch size),\n",
        "                    - vocab_size (size of vocabulary),\n",
        "                    - embedding_size (same as in embedding),\n",
        "                    - target_dim (final prediction size)\n",
        "        \"\"\"\n",
        "        self.input_size = param_dict.get('input_size', 0)\n",
        "        self.vocab_size = param_dict.get('vocab_size')\n",
        "        self.embedding_dim = param_dict.get('embedding_dim', 100)\n",
        "        self.target_dim = param_dict.get('target_dim')\n",
        "        \n",
        "class TextModel(nn.Module):\n",
        "    \"\"\"\n",
        "        Class with LSTM model as described in baseline in in Memotion Analisys \n",
        "        task summary (https://arxiv.org/pdf/2008.03781.pdf)\n",
        "    \"\"\"\n",
        "    def __init__(self, model_param: ModelParam):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                model_param (ModelParam) - parameters for model in ModelParam class wrapper\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(model_param.vocab_size, model_param.embedding_dim)\n",
        "        self.conv = nn.Conv1d(64, 100, 4)\n",
        "        self.max_pool = nn.MaxPool1d(2)\n",
        "        self.lstm = nn.LSTM(48, 16, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=3200, out_features=4, bias=True)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.flatt = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            Perform training on element x\n",
        "            Args:\n",
        "                x - elem to train on\n",
        "            Returns:\n",
        "                tensor of predictions\n",
        "        \"\"\"\n",
        "        features = self.embedding(x)\n",
        "        features = F.relu(features)\n",
        "        features = self.conv(features)\n",
        "        features = self.max_pool(features)\n",
        "        features, hidden = self.lstm(features)\n",
        "        features = self.flatt(features)\n",
        "        features = self.fc(features)\n",
        "        features = self.sigm(features)\n",
        "        return features"
      ],
      "metadata": {
        "id": "BFVgxQBzX7Fe"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trenowanie modelu LSTM"
      ],
      "metadata": {
        "id": "ca2gNerQ4OTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text model train - LSTM\n",
        "\n",
        "if use_trained_model==False:\n",
        "    model_param = ModelParam(\n",
        "    param_dict=dict(\n",
        "        vocab_size=len(text_field.vocab),\n",
        "        input_size=64,\n",
        "        embedding_dim=100,\n",
        "        target_dim=4\n",
        "        )\n",
        "    )\n",
        "    train_text_model = TextModel(model_param).cuda()\n",
        "    \n",
        "    loss_function = nn.BCELoss()\n",
        "    optimizer = optim.Adam(train_text_model.parameters(), lr=0.0002)\n",
        "    best_model_wts = copy.deepcopy(train_text_model.state_dict())\n",
        "    best_val_f1 = -1\n",
        "    epoch_count = 0\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    val_acc = []\n",
        "    for epoch in range(text_epochs):\n",
        "        train_text_model.train()\n",
        "        clear_text_f1_data()\n",
        "        b_num = 0\n",
        "        epoch_num = 0\n",
        "        epoch_losses = []\n",
        "        k = 0\n",
        "        for batch in train_iter:\n",
        "            optimizer.zero_grad()\n",
        "            prediction = train_text_model(batch.text.T.cuda())\n",
        "            batch_label = batch.label.to(torch.float)\n",
        "            labels = text_classes_convert(batch_label).cuda()\n",
        "            preds = torch.flatten(torch.max(prediction, 1)[1]).float().cuda()\n",
        "            # print(prediction.shape)\n",
        "            # print(batch.label.shape)\n",
        "            loss = loss_function(torch.squeeze(prediction), torch.squeeze(batch.label.to(torch.float)).cuda())\n",
        "            #update_text_f1_data(preds.squeeze().tolist(), labels.squeeze().tolist())\n",
        "            # print(prediction)\n",
        "            # print(batch.label)\n",
        "            # print(preds)\n",
        "            # print(labels)\n",
        "            update_text_f1_data(preds.tolist(), labels.tolist())\n",
        "            loss.backward()\n",
        "            epoch_losses.append(loss.item())\n",
        "            optimizer.step()\n",
        "            k = k+1\n",
        "        \n",
        "        losses.append(sum(epoch_losses)/k)\n",
        "        train_text_macro_f1 = calculate_text_macro_f1()\n",
        "        \n",
        "        train_text_model.eval()\n",
        "        clear_text_f1_data()\n",
        "        val_epoch_losses = []\n",
        "        k = 0\n",
        "        total_acc_val = 0\n",
        "        \n",
        "        test_iter_elements_number = 0\n",
        "        for batch in test_iter:\n",
        "            with torch.no_grad():\n",
        "                optimizer.zero_grad()\n",
        "                prediction = train_text_model(batch.text.T.cuda())\n",
        "                batch.label = batch.label.to(torch.float)\n",
        "                labels = text_classes_convert(batch.label).cuda()\n",
        "                # preds = torch.flatten(torch.max(prediction, 1)[1]).float().cuda()\n",
        "                _, preds = torch.max(prediction, 1)\n",
        "                # print(preds)\n",
        "                # print(preds2)\n",
        "                loss = loss_function(torch.squeeze(prediction), torch.squeeze(batch.label.to(torch.float)).cuda())\n",
        "                #update_text_f1_data(preds.squeeze().tolist(), labels.squeeze().tolist())\n",
        "                # print(prediction)\n",
        "                # print(batch.label)\n",
        "                # print(preds)\n",
        "                # print(labels)\n",
        "                \n",
        "                test_iter_elements_number += len(labels)\n",
        "                acc = (preds == labels).sum().item()\n",
        "                # print(acc)\n",
        "                # print(loss)\n",
        "                total_acc_val += acc\n",
        "                val_epoch_losses.append(loss.item())\n",
        "                update_text_f1_data(preds.tolist(), labels.tolist())\n",
        "                k = k+1\n",
        "\n",
        "        # print(total_acc_val)\n",
        "        # print(test_iter_elements_number)\n",
        "        val_acc.append(total_acc_val / test_iter_elements_number)\n",
        "        val_losses.append(sum(val_epoch_losses)/k)\n",
        "        val_text_macro_f1 = calculate_text_macro_f1()\n",
        "        if val_text_macro_f1 > best_val_f1:\n",
        "            best_val_f1 = val_text_macro_f1\n",
        "            best_model_wts = copy.deepcopy(train_text_model.state_dict())\n",
        "        else :\n",
        "            train_text_model.load_state_dict(best_model_wts)\n",
        "        print()\n",
        "        print(\"Epoch {} result: \".format(epoch))\n",
        "        print(\"F1 score: {:.4f} (train), {:.4f} (val)\".format(train_text_macro_f1, val_text_macro_f1))\n",
        "        print('-' * 10)\n",
        "        print()\n",
        "    print(f\"Best val f1 score: {best_val_f1}\")\n",
        "    if save_model:\n",
        "        torch.save(best_model_wts, text_save_model_path)\n",
        "        print(\"Text model saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "yjVN5puCYh1G",
        "outputId": "49681177-7269-4574-87f2-f9754ac668ca"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-655f077c0c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testowanie wytrenowanych modeli"
      ],
      "metadata": {
        "id": "-KspV4xGLPv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie danych do testowania (w wypadku jednoczesnego trenowania i testowania wczytywany jest drugi raz ten sam zbiór)"
      ],
      "metadata": {
        "id": "qfEtq3Ee5NUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset with data for Memotion Analisys Kaggle for tests\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path, low_data_mode=False, debug=False):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "            csv_path (string): path to csv file with data \n",
        "            low_data_mode (boolean): low data mode for testing \n",
        "            debug (boolean): enable debug options\n",
        "        \"\"\"\n",
        "        self.debug = debug\n",
        "        # Read the csv_file\n",
        "        if low_data_mode==True:\n",
        "          self.data_info = pd.read_csv(csv_path, header = 6952)\n",
        "        else:\n",
        "          self.data_info = pd.read_csv(csv_path, header = 3)\n",
        "\n",
        "        # Column containing image names\n",
        "        self.image_arr = np.asarray(self.data_info.iloc[:, 1])\n",
        "        # Columns containing emotions classification\n",
        "        self.humour_arr = np.asarray(self.data_info.iloc[:, 4])\n",
        "        self.sarcasm_arr = np.asarray(self.data_info.iloc[:, 5])\n",
        "        self.offensive_arr = np.asarray(self.data_info.iloc[:, 6])\n",
        "        self.motivational_arr = np.asarray(self.data_info.iloc[:, 7])\n",
        "        \n",
        "        # Transforms performed on loaded image\n",
        "        self.data_transforms = transforms.Compose([\n",
        "                                      transforms.Resize((224, 224)),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        \n",
        "        # Array with class vectors for each image\n",
        "        self.labels = []\n",
        "        self.text_arr = self.data_info.iloc[:, 3]\n",
        "        raw_df = []\n",
        "\n",
        "        for i in range(len(self.text_arr)):\n",
        "            raw_df.append([str(self.text_arr[i])])\n",
        "        self.df = pd.DataFrame(raw_df[:-2], columns=['text'])\n",
        "\n",
        "        self.text_field = Field(\n",
        "            sequential=True,\n",
        "            tokenize='basic_english', \n",
        "            fix_length=50, \n",
        "            lower=True\n",
        "        )\n",
        "        # prepocess\n",
        "        preprocessed_text = self.df['text'].apply(\n",
        "            lambda x: self.text_field.preprocess(x)\n",
        "        )\n",
        "        # load fastext simple embedding with 100d\n",
        "        self.text_field.build_vocab(\n",
        "            preprocessed_text, \n",
        "            vectors='glove.6B.100d'\n",
        "        )\n",
        "\n",
        "        class DataFrameDataset(Dataset):\n",
        "            \"\"\"\n",
        "                    Dataset created drom pandas DataFrame with text data from Kaggle for \n",
        "                    Memotion Analisys task\n",
        "            \"\"\"\n",
        "            def __init__(self, df: pd.DataFrame, fields: list):\n",
        "                super(DataFrameDataset, self).__init__(\n",
        "                    [\n",
        "                        Example.fromlist(list(r), fields) for i, r in df.iterrows()\n",
        "                    ], \n",
        "                    fields\n",
        "                )\n",
        "        self.text_dataset = DataFrameDataset(\n",
        "            df=df, \n",
        "            fields=(\n",
        "                ('text', text_field),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.text_iter = BucketIterator(\n",
        "            dataset=train_dataset, \n",
        "            batch_size=1\n",
        "        )\n",
        "\n",
        "        # Mapping word classification to 4 numeric classes\n",
        "        for index in range(len(self.humour_arr)):\n",
        "          humour_value = class_humour_weights[self.humour_arr[index]]\n",
        "          sarcasm_value = class_sarcasm_weights[self.sarcasm_arr[index]]\n",
        "          offensive_value = class_offensive_weights[self.offensive_arr[index]]\n",
        "          motivational_value = class_motivational_weights[self.motivational_arr[index]]\n",
        "\n",
        "          if humour_value > sarcasm_value:\n",
        "            if humour_value > offensive_value:\n",
        "              if humour_value > motivational_value:\n",
        "                var = 0\n",
        "              else:\n",
        "                var = 3 \n",
        "            else:\n",
        "              if offensive_value > motivational_value:\n",
        "                var = 2\n",
        "              else: \n",
        "                var = 3\n",
        "          else:\n",
        "            if sarcasm_value > offensive_value:\n",
        "              if sarcasm_value > motivational_value:\n",
        "                var = 1\n",
        "              else:\n",
        "                var = 3\n",
        "            else: \n",
        "              if offensive_value > motivational_value: \n",
        "                var = 2\n",
        "              else:\n",
        "                var = 3\n",
        "\n",
        "          # Creating class vector\n",
        "          lab = [0.0, 0.0, 0.0, 0.0]\n",
        "          lab[var] = 1.0\n",
        "          \n",
        "          # Adding new image class vector to labels array\n",
        "          self.labels.append(lab) \n",
        "\n",
        "        # Calculate of dataset\n",
        "        self.data_len = len(self.data_info.index)\n",
        "        \n",
        "        # Set correct path to images\n",
        "        self.image_arr = images_dir + self.image_arr\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          index (int): index of item to get  \n",
        "\n",
        "        Returns:\n",
        "          Tuple of image, text and class vector as tensors\n",
        "        \"\"\"\n",
        "        img_as_img = None\n",
        "        single_image_name = None\n",
        "\n",
        "        try:\n",
        "          # Get image name from pandas df\n",
        "          single_image_name = self.image_arr[index]\n",
        "\n",
        "          # # Open image with PIL and convert to RGB image\n",
        "          img = Image.open(single_image_name).convert('RGB')\n",
        "          if self.debug==True:\n",
        "            print('1:', img)\n",
        "\n",
        "          # Transform image and convert to tensor\n",
        "          img_as_tensor = self.data_transforms(img)\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('2:', img_as_tensor)\n",
        "\n",
        "          # Get class vector of the image from labels array\n",
        "          label = self.labels[index]\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('3:',label)\n",
        "\n",
        "          # Convert class vector to tensor\n",
        "          label = torch.as_tensor(label)\n",
        "          \n",
        "          if self.debug==True:\n",
        "            print('4:',label)\n",
        "\n",
        "          text_data = next(iter(self.text_iter)).text\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('5:',text_data)\n",
        "\n",
        "          return (img_as_tensor, text_data, label)\n",
        "        except:\n",
        "          print(\"Image loading error for:\",single_image_name)\n",
        "          return ('ERROR', torch.tensor([-1]))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data_len    \n"
      ],
      "metadata": {
        "id": "ciwGsrcWkQDA"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjalizacja testowego zbioru danych"
      ],
      "metadata": {
        "id": "jBdtZYt67SpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TestDataset('MemotionAnalysis/labels.csv', low_data_mode=False, debug=False)\n",
        "\n",
        "# Loading dataset into DataLoader and setting batch_size\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "AR7pRxEUx2XD"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdzenie poprawnosci wczytania danych"
      ],
      "metadata": {
        "id": "OFphol1t7YL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, text, label = next(iter(dataloader))\n",
        "print(image.shape)\n",
        "print(text.shape)\n",
        "print(label.shape)"
      ],
      "metadata": {
        "id": "IPTM_Gd6yZeM",
        "outputId": "34fd808e-bdd0-403d-bb82-5204ff2ffb73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 224, 224])\n",
            "torch.Size([4, 64, 1])\n",
            "torch.Size([4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjalizacja modeli dla tekstu i obrazu: w razie uczenia i testowania użyte są modele z treningu, w razie użycia wcześniej wyternowanych zbiorów zostają one wczytane w podanych na poczatkui pliku ścieżek"
      ],
      "metadata": {
        "id": "9X09079j7cW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_model, image_model = None, None\n",
        "if use_trained_model:\n",
        "    text_model_dict = torch.load(text_load_model_path, map_location=torch.device('cpu') if not use_gpu else None)\n",
        "    image_model_dict = torch.load(image_load_model_path, map_location=torch.device('cpu') if not use_gpu else None)\n",
        "    vgg = models.vgg16_bn()\n",
        "    vgg.load_state_dict(torch.load(\"/content/vgg16_bn.pth\")) \n",
        "    image_model = custom_vgg(vgg16)\n",
        "    model_param = ModelParam(\n",
        "    param_dict=dict(\n",
        "        vocab_size=len(text_field.vocab),\n",
        "        input_size=50,\n",
        "        embedding_dim=100,\n",
        "        target_dim=4\n",
        "        )\n",
        "    )\n",
        "    text_model = TextModel(model_param)\n",
        "    image_model.load_state_dict(image_model_dict)\n",
        "    text_model.load_state_dict(text_model_dict)\n",
        "else:\n",
        "    text_model = train_text_model\n",
        "    image_model = vgg"
      ],
      "metadata": {
        "id": "a9_dXxxurkAt"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdzenie poprawności wczytania modeli"
      ],
      "metadata": {
        "id": "jZmrBKLZ7uEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_model)\n",
        "print(image_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0CymgUquPvY",
        "outputId": "72ab9d57-cad2-4e12-ccd6-a9dcd91781f1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextModel(\n",
            "  (embedding): Embedding(13676, 100)\n",
            "  (conv): Conv1d(64, 100, kernel_size=(4,), stride=(1,))\n",
            "  (max_pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (lstm): LSTM(48, 16, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=3200, out_features=4, bias=True)\n",
            "  (sigm): Sigmoid()\n",
            "  (flatt): Flatten(start_dim=1, end_dim=-1)\n",
            ")\n",
            "custom_vgg(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (pooling): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc): Linear(in_features=25088, out_features=4, bias=True)\n",
            "  (sigm): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja funkcji pomocniczej konwertującej klasy jako indeksy w macierzy do listy z indeksami na których występują dane, przykład:\n",
        "```\n",
        "[[1, 0, 0, 0],\n",
        " [0, 0, 1, 0],\n",
        " [0, 1, 0, 0],  \n",
        " [0, 0, 0, 1]]  = [0, 2, 1 ,3]\n",
        "```\n",
        "Funckja potrzebna jest przy ewaluacji"
      ],
      "metadata": {
        "id": "XSdKJpu18S1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels_as_indices(labels_data):\n",
        "    \"\"\"\n",
        "    Converts matrix of indices (classes) to list with indices of value ex:\n",
        "        [[1, 0, 0, 0],\n",
        "         [0, 0, 1, 0],\n",
        "         [0, 1, 0, 0],  \n",
        "         [0, 0, 0, 1]]  = [0, 2, 1 ,3]\n",
        "        Args:\n",
        "            classes (list): list of lists for classes which will be converted\n",
        "    \"\"\" \n",
        "    return torch.flatten(torch.max(labels_data, 1)[1]).float()"
      ],
      "metadata": {
        "id": "kIIs7eKhas9D"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdefiniowanie funkcji predykującej"
      ],
      "metadata": {
        "id": "hX0lb_Wa9ZR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(image_data, text_data):\n",
        "    \"\"\"\n",
        "        Predict class of a memes\n",
        "        Args:\n",
        "            image_data - data of images to predict\n",
        "            text_data - data of text to predict\n",
        "        Returns:\n",
        "            list of predicitions\n",
        "    \"\"\"\n",
        "    image_pred = image_model(image_data)\n",
        "    text_pred = torch.squeeze(text_model(text_data))\n",
        "    preds = image_pred*0.3 + text_pred*0.7\n",
        "    images_preds = get_labels_as_indices(image_pred)\n",
        "    text_preds = get_labels_as_indices(text_pred)\n",
        "    print(f\"Image pred: {images_preds}\")\n",
        "    print(f\"Text pred: {text_preds}\")\n",
        "    preds = get_labels_as_indices(preds)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "1dz2aOItwvhp"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przykładowa predykcja danych"
      ],
      "metadata": {
        "id": "5DmIj0bB9c1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image_inputs, image_classes = next(iter(dataloader))\n",
        "# text_data = next(iter(train_iter))\n",
        "\n",
        "image_data, text_data, label = next(iter(dataloader))\n",
        "\n",
        "preds = predict(image_data, torch.squeeze(text_data))\n",
        "\n",
        "print(preds)\n",
        "print(get_labels_as_indices(label))\n",
        "# print(get_labels_as_indices(text_data.label))"
      ],
      "metadata": {
        "id": "kITwhpuTxgX6",
        "outputId": "afa02b79-9bad-4baf-826c-9feac6fd0bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-ba9e1ecf6afa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-1324bfad6eca>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(image_data, text_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpredicitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimage_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtext_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_pred\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext_pred\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-9558c4f54629>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mto\u001b[0m \u001b[0mperform\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \"\"\"\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ]
    }
  ]
}