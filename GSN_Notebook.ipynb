{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSN_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ded0605b90cb490fa695015fed4c81a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_63e5962907ed4ec395edc58a6e3ac3eb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2848f05fb494988b57f84fb6360dc46",
              "IPY_MODEL_bde4d6ffbcf942a2987e0e4d885af263",
              "IPY_MODEL_456fb4cc92cf4b0f8240770d814ed100"
            ]
          }
        },
        "63e5962907ed4ec395edc58a6e3ac3eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2848f05fb494988b57f84fb6360dc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f1db7fb69824015959d2ee71ffe5b25",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91181203f8344fcd9dcf611794e89d94"
          }
        },
        "bde4d6ffbcf942a2987e0e4d885af263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9968901257494eae87a5f868ad53f01b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25db280dc6414db89a3df20df9229fe6"
          }
        },
        "456fb4cc92cf4b0f8240770d814ed100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5976646f909d499f9799d85744b9d153",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 20.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed28b58edb534826883a735348fa1b43"
          }
        },
        "5f1db7fb69824015959d2ee71ffe5b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91181203f8344fcd9dcf611794e89d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9968901257494eae87a5f868ad53f01b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25db280dc6414db89a3df20df9229fe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5976646f909d499f9799d85744b9d153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed28b58edb534826883a735348fa1b43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "277249cef9a64fa18c1adc73cd98985f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6bf1f7b018b549148731e80c4697c796",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_516bd13dd963475fbd3a4d0677765dc9",
              "IPY_MODEL_1a94ece28ecc4e139deb441e8a89dcb6",
              "IPY_MODEL_bd7fe67cdc334b9ca65a902e1ce94a3f"
            ]
          }
        },
        "6bf1f7b018b549148731e80c4697c796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "516bd13dd963475fbd3a4d0677765dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6fc9aa1c55114b6b9f74732f6c34d661",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f9da55fc9bf4052bb3e4e8dfa412944"
          }
        },
        "1a94ece28ecc4e139deb441e8a89dcb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc98adaac7d1423eb05a64ae3a8ddb56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9bd034f005ca4f289fa8c53fed9f2c3f"
          }
        },
        "bd7fe67cdc334b9ca65a902e1ce94a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cd62ac6962df47dfaaec7bedac931673",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 416M/416M [00:08&lt;00:00, 57.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d69323ab281f4cacb93913a8e1f05a74"
          }
        },
        "6fc9aa1c55114b6b9f74732f6c34d661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f9da55fc9bf4052bb3e4e8dfa412944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc98adaac7d1423eb05a64ae3a8ddb56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9bd034f005ca4f289fa8c53fed9f2c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd62ac6962df47dfaaec7bedac931673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d69323ab281f4cacb93913a8e1f05a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcf5022fa1644638a76b06626dd24344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75ee056dbf3b4c2e8cc4323d2f0ceebb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_60ff5baddb9c469e903e01025ca67d23",
              "IPY_MODEL_32d2451aaeb149a1afd8b9bf546543f0",
              "IPY_MODEL_b3d854d137f244b4a47c1c04c53ee076"
            ]
          }
        },
        "75ee056dbf3b4c2e8cc4323d2f0ceebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60ff5baddb9c469e903e01025ca67d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c1f9682c49cf48368b6f9639a9e8883f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5f50e4a2e9144e5b55ec58f0024e0ad"
          }
        },
        "32d2451aaeb149a1afd8b9bf546543f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a6ead3bb023e42a0b0a0a53357850092",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ffd0eede358f4410a77f63f00649ed69"
          }
        },
        "b3d854d137f244b4a47c1c04c53ee076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_797d83249c694a4291479b9ef3ee12f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 208k/208k [00:00&lt;00:00, 4.71MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3f985b57899449b8dfa1c921bf30c54"
          }
        },
        "c1f9682c49cf48368b6f9639a9e8883f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5f50e4a2e9144e5b55ec58f0024e0ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6ead3bb023e42a0b0a0a53357850092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ffd0eede358f4410a77f63f00649ed69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "797d83249c694a4291479b9ef3ee12f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3f985b57899449b8dfa1c921bf30c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf1d08cca3364925beb63b48502bd84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_44a45dd69fd041aa93baf37d28081308",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09789d8c27924d358e9c89b1950b5817",
              "IPY_MODEL_f68358c36d474f4ab9e1230e6ac09353",
              "IPY_MODEL_ee39cf7dd28f41f381f2647dfb1cf264"
            ]
          }
        },
        "44a45dd69fd041aa93baf37d28081308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09789d8c27924d358e9c89b1950b5817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4dfef9ab91554547b856ade617af1bbf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61b0ad20a7774ed4b91ed164ac305522"
          }
        },
        "f68358c36d474f4ab9e1230e6ac09353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_331a44d92aa14d1394b72c0ab3b0fec3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eca16d1472d149b295f64b470edc2b5e"
          }
        },
        "ee39cf7dd28f41f381f2647dfb1cf264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0eb8aad7244f450896e7e8054a70d73a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29.0/29.0 [00:00&lt;00:00, 863B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb73f69ad4664c16b6c1246fe5171c91"
          }
        },
        "4dfef9ab91554547b856ade617af1bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61b0ad20a7774ed4b91ed164ac305522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "331a44d92aa14d1394b72c0ab3b0fec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eca16d1472d149b295f64b470edc2b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0eb8aad7244f450896e7e8054a70d73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb73f69ad4664c16b6c1246fe5171c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8bdbfc32229040d9a37835fcda313058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ae6e9e65783541fda60beae7a77f5b6e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e34cbd6fc3af417bab975cdc8b025f7e",
              "IPY_MODEL_6f300cf192ea45c388dfcb616fee9513",
              "IPY_MODEL_b3ff0e13fec04dc29c467a96db3cc615"
            ]
          }
        },
        "ae6e9e65783541fda60beae7a77f5b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e34cbd6fc3af417bab975cdc8b025f7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c953ff44847848e39436b0f7b80dddea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_864f55b6d484451bb284d41a60c8a657"
          }
        },
        "6f300cf192ea45c388dfcb616fee9513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_082801b543f343f5a20b79a5a9e62e65",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435797,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435797,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb1a4a5680f14e5b8fef1aba59dcc398"
          }
        },
        "b3ff0e13fec04dc29c467a96db3cc615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db8cd737924b4296a5f22bf8df0c87a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 426k/426k [00:00&lt;00:00, 8.31MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0e34374d9414286893e18ef8cdb1d37"
          }
        },
        "c953ff44847848e39436b0f7b80dddea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "864f55b6d484451bb284d41a60c8a657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "082801b543f343f5a20b79a5a9e62e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb1a4a5680f14e5b8fef1aba59dcc398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db8cd737924b4296a5f22bf8df0c87a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0e34374d9414286893e18ef8cdb1d37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tyluuus/MemotionAnalysis/blob/udpates/GSN_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSN21Z Projekt "
      ],
      "metadata": {
        "id": "1rBvyUIKhIm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celem projektu będzie realizacja tasku B znajdującego się pod challengem Memotion na platformie Kaggle: https://www.kaggle.com/williamscott701/memotion-dataset-7k\n",
        "\n",
        "\n",
        "Treść zadania: *Task B- Humor Classification: Given an Internet meme, the system has to identify the type of humor expressed. The categories are sarcastic, humorous, and offensive meme. If a meme does not fall under any of these categories, then it is marked as another meme. A meme can have more than one category.*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HzZuFchUhK59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: W datasecie znajduje się 6992 memów pobranych z platformy reddit oraz oznaczonych z wykorzystaniem usługi Amazon Mechanical Turk. Oznaczenia znajdują się w pliku csv, który zawiera: \n",
        "1.   Nazwę pliku z memem\n",
        "2.   Tekst uzyskany z wykorzystaniem OCR\n",
        "3.   Tekst poprawiony\n",
        "4.   Klasa humorystyczna\n",
        "5.   Klasa sarkastyczna\n",
        "6.   Klasa ofensywna\n",
        "7.   Klasa motywacyjna\n"
      ],
      "metadata": {
        "id": "E0WtWWldhMuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie do użycia kodu"
      ],
      "metadata": {
        "id": "Erq9o3G_eKH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podłączenie notatnika do dysku Google - potrzebne przy zapisie checkpoint'ów i słowników stanów "
      ],
      "metadata": {
        "id": "lGx8n6QjeUnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "mXDbxxTh3ydL",
        "outputId": "3ded4066-bc9e-4639-d2ca-00fbcc35c9d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sklonowanie repozytorium z githuba - pobranie plików składających się na dataset - zdjęć oraz pliku csv, pobierane są również pliki niezbędne do wczytania modelu sieci"
      ],
      "metadata": {
        "id": "ANEOn1zMenoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Tyluuus/MemotionAnalysis.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo0BnasUhoIN",
        "outputId": "b2b6d460-ea63-4d3e-f045-2c7ed64e6998"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MemotionAnalysis'...\n",
            "remote: Enumerating objects: 7115, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 7115 (delta 30), reused 3 (delta 1), pack-reused 7063\u001b[K\n",
            "Receiving objects: 100% (7115/7115), 862.64 MiB | 40.49 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n",
            "Checking out files: 100% (13992/13992), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalacja pakietu `transformers` użytym przy klasyfikatorze BERT"
      ],
      "metadata": {
        "id": "_Q1Kc-vxe0Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDlvwtuTQCHi",
        "outputId": "f82142c3-4541-4b8f-c1b0-0033dd471eae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 74.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 77.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importy wszystkich bibliotek wykorzystywanych w projekcie"
      ],
      "metadata": {
        "id": "YgxEv063e8OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchtext.legacy.data import Dataset, Example, Field\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "4aknJXfFqGaI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przełączenie na używanie GPU w projekcie jeżeli jest to możliwe"
      ],
      "metadata": {
        "id": "rZsA7tHHfamw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to using GPU\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print(\"Using CUDA\")"
      ],
      "metadata": {
        "id": "hEbiFkj3tuUC",
        "outputId": "46b39b7c-23d3-4f08-954d-e496bdb0083e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Określenie:\n",
        "- czy używany jest wytrenowany model, \n",
        "- czy wytrenowane modele powinny być zapisane,\n",
        "- czy wykonany ma być _tranfer learning_,\n",
        "- ścieżek do zapisu i odczytu modeli"
      ],
      "metadata": {
        "id": "ft6F-9Qyfh9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use trained model variable\n",
        "use_trained_model = False\n",
        "save_model = True\n",
        "image_model_continue_training = False\n",
        "image_save_model_path = './drive/MyDrive/GSN_dataset/memotion_images_model_5_epoch_vgg19.pt'\n",
        "text_save_model_path = './drive/MyDrive/GSN_dataset/memotion_text_model_bert_10_epochs.pt'\n",
        "image_load_model_path = './MemotionAnalysis/memotion_images_model_1_epoch.pt'\n",
        "text_load_model_path = './MemotionAnalysis/memotion_text_model_1_epoch.pt'\n",
        "image_checkpoint_path = './drive/MyDrive/GSN_dataset/checkpoint.pt'\n",
        "text_checkpoint_path = './drive/MyDrive/GSN_dataset/memotion_text_model_bert_10_epochs_checkpoint.pt'"
      ],
      "metadata": {
        "id": "B6v2cyDH1dKR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Określenie liczby epok dla trenowania gałęzi tekstu i obrazu oraz rozmiaru batchy - _same_batch_size_ jest wielkością dla jednoczesnego trenowania obrazu i testu z tą samą wielkością batcha"
      ],
      "metadata": {
        "id": "ube5qF9Fh-Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_epochs = 200 # 250\n",
        "image_epochs = 5 # 20\n",
        "text_batch_size = 2\n",
        "image_batch_size = 16\n",
        "\n",
        "same_batch_size = 32"
      ],
      "metadata": {
        "id": "XYO4NllQebSK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pobranie pretrenowanego modelu dla tekstu w razie potrzeby"
      ],
      "metadata": {
        "id": "lnHS0maKjChW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download text model for 300 epochs\n",
        "!gdown --id 1-9UMs4-3DO0acalwBUMf7rKHXBg1qV8n"
      ],
      "metadata": {
        "id": "6-kPaMfNz6Q0",
        "outputId": "a07eb062-f51e-46eb-932e-774d6f240760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-9UMs4-3DO0acalwBUMf7rKHXBg1qV8n\n",
            "To: /content/memotion_text_model_300_epoch.pt\n",
            "\r  0% 0.00/5.49M [00:00<?, ?B/s]\r100% 5.49M/5.49M [00:00<00:00, 49.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Domyślnie trenowanymi modelami są VGG16 oraz LSTM. Wybór innych modeli możliwy jest w sekcjach odpowiedzialnych za poszczególne gałęzie."
      ],
      "metadata": {
        "id": "yHInTc-vN8BF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gałąź przetwarzania obrazu"
      ],
      "metadata": {
        "id": "b7Yd9DTVXfvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utworzenie zbioru danych na podstawie danych z Kaggle"
      ],
      "metadata": {
        "id": "6953TJgPGwop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie danych na zaimplementowanym obiekcie Dataset"
      ],
      "metadata": {
        "id": "h86FE3jYo3d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset designed to load data for Memotion Analisys Task from correspondig\n",
        "        Kaggle and assign weights for classes from task csv file\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path, low_data_mode=False, debug=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          csv_path (string): path to csv file with data \n",
        "          low_data_mode (boolean): low data mode for testing \n",
        "          debug (boolean): enable debug options\n",
        "        \"\"\"\n",
        "        # If system is in debug mode\n",
        "        self.debug = debug\n",
        "\n",
        "\n",
        "        # Read the csv_file\n",
        "        if low_data_mode==True:\n",
        "          self.data_info = pd.read_csv(csv_path, header = 6952)\n",
        "        else:\n",
        "          self.data_info = pd.read_csv(csv_path, header = 3)\n",
        "\n",
        "        # Column containing image names\n",
        "        self.image_arr = np.asarray(self.data_info.iloc[:, 1])\n",
        "\n",
        "        # Columns containing emotions classification\n",
        "        self.humour_arr = np.asarray(self.data_info.iloc[:, 4])\n",
        "        self.sarcasm_arr = np.asarray(self.data_info.iloc[:, 5])\n",
        "        self.offensive_arr = np.asarray(self.data_info.iloc[:, 6])\n",
        "        self.motivational_arr = np.asarray(self.data_info.iloc[:, 7])\n",
        "        \n",
        "\n",
        "\n",
        "        # Transforms performed on loaded image\n",
        "        self.data_transforms = transforms.Compose([\n",
        "                                      transforms.Resize((224, 224)),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        \n",
        "        # Array with class vectors for each image\n",
        "        self.labels = []\n",
        "\n",
        "        # Mapping word classification to 4 numeric classes\n",
        "        for index in range(len(self.humour_arr)):\n",
        "          humour_value = class_humour_weights[self.humour_arr[index]]\n",
        "          sarcasm_value = class_sarcasm_weights[self.sarcasm_arr[index]]\n",
        "          offensive_value = class_offensive_weights[self.offensive_arr[index]]\n",
        "          motivational_value = class_motivational_weights[self.motivational_arr[index]]\n",
        "\n",
        "          if humour_value > sarcasm_value:\n",
        "            if humour_value > offensive_value:\n",
        "              if humour_value > motivational_value:\n",
        "                var = 0\n",
        "              else:\n",
        "                var = 3 \n",
        "            else:\n",
        "              if offensive_value > motivational_value:\n",
        "                var = 2\n",
        "              else: \n",
        "                var = 3\n",
        "          else:\n",
        "            if sarcasm_value > offensive_value:\n",
        "              if sarcasm_value > motivational_value:\n",
        "                var = 1\n",
        "              else:\n",
        "                var = 3\n",
        "            else: \n",
        "              if offensive_value > motivational_value: \n",
        "                var = 2\n",
        "              else:\n",
        "                var = 3\n",
        "\n",
        "          # Creating class vector\n",
        "          lab = [0.0, 0.0, 0.0, 0.0]\n",
        "          lab[var] = 1.0\n",
        "          \n",
        "          # Adding new image class vector to labels array\n",
        "          self.labels.append(lab) \n",
        "\n",
        "        # Calculate of dataset\n",
        "        self.data_len = len(self.data_info.index)\n",
        "        \n",
        "        # Set correct path to images\n",
        "        self.image_arr = images_dir + self.image_arr\n",
        "\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          index (int): index of item to get  \n",
        "\n",
        "        Returns:\n",
        "          Tuple of image, text and class vector as tensors\n",
        "        \"\"\"\n",
        "        img_as_img = None\n",
        "        single_image_name = None\n",
        "\n",
        "\n",
        "        try:\n",
        "          # Get image name from pandas df\n",
        "          single_image_name = self.image_arr[index]\n",
        "\n",
        "          # # Open image with PIL and convert to RGB image\n",
        "          img = Image.open(single_image_name).convert('RGB')\n",
        "          if self.debug==True:\n",
        "            print('1:', img)\n",
        "\n",
        "          # Transform image and convert to tensor\n",
        "          img_as_tensor = self.data_transforms(img)\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('2:', img_as_tensor)\n",
        "\n",
        "          # Get class vector of the image from labels array\n",
        "          img_label = self.labels[index]\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('3:',img_label)\n",
        "\n",
        "          # Convert class vector to tensor\n",
        "          img_label = torch.as_tensor(img_label)\n",
        "          \n",
        "          if self.debug==True:\n",
        "            print('4:',img_label)\n",
        "\n",
        "          return (img_as_tensor, img_label)\n",
        "\n",
        "        except:\n",
        "          print(\"Image loading error for:\",single_image_name)\n",
        "          return ('ERROR', torch.tensor([-1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "metadata": {
        "id": "qAerCA5Gqd_D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapowanie opisów z pliku csv na liczby zgodnie z wagą jaką reprezentują dla danego odczucia"
      ],
      "metadata": {
        "id": "dr0z-b0ApZTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionaries for mapping word classification\n",
        "class_humour_weights = {\"hilarious\": 3, \"not_funny\": 0, \"very_funny\": 2, \"funny\": 1}\n",
        "class_sarcasm_weights = {\"general\": 1, \"not_sarcastic\": 0, \"twisted_meaning\": 2, \"very_twisted\": 3}\n",
        "class_offensive_weights = {\"not_offensive\": 0, \"slight\": 1, \"very_offensive\": 2, \"hateful_offensive\": 3}\n",
        "class_motivational_weights = {\"not_motivational\": 0, \"motivational\": 1}\n",
        "\n",
        "# Directory containing images\n",
        "images_dir = \"./MemotionAnalysis/images/\""
      ],
      "metadata": {
        "id": "v6vAEOVpsjht"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie danych oraz utworzenie z nich dataloader'a"
      ],
      "metadata": {
        "id": "6V9Ac1BYrqQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading images into custom dataset\n",
        "dataset = MyCustomDataset('MemotionAnalysis/labels.csv', low_data_mode=False)\n",
        "\n",
        "# Loading dataset into DataLoader and setting batch_size\n",
        "b_size = same_batch_size if use_trained_model else image_batch_size\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=b_size, shuffle=False, num_workers=1)\n",
        "dataset_size = len(dataloader)"
      ],
      "metadata": {
        "id": "QyONsUPTtxnL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdzenie wczytywanych danych"
      ],
      "metadata": {
        "id": "7QvdybtDt0MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check loaded data\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "def show_databatch(inputs, classes):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    imshow(out, title=[classes])\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloader))\n",
        "show_databatch(inputs, classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "XQ6mkihFuZ0B",
        "outputId": "0aeb3f08-bfa1-4995-d3db-9339a52f087f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definicja modelu i uczenie"
      ],
      "metadata": {
        "id": "NtI-EnR9hPkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wybór modelu obrazu z dwóch przetestowanych modeli VGG16 i VGG19"
      ],
      "metadata": {
        "id": "9_13Es0Xv1uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# avaliable models: vgg_16, vgg_19\n",
        "image_model_type = 'vgg_16' # 'vgg_19'"
      ],
      "metadata": {
        "id": "Orvtw4N67SIt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pobranie słownika stanu dla pretrenowanego VGG 16"
      ],
      "metadata": {
        "id": "RtjesP7Dv8pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading pretrained vgg16\n",
        "\n",
        "!gdown --id 1f3iqJY2NDTsJP2_BFmtk5WtcfAW9nwdl"
      ],
      "metadata": {
        "id": "9lJ81u6u4Em2",
        "outputId": "7f66f519-8293-474d-cee4-b672b4cb2045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f3iqJY2NDTsJP2_BFmtk5WtcfAW9nwdl\n",
            "To: /content/vgg16_bn.pth\n",
            "100% 554M/554M [00:05<00:00, 98.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zainicjowanie modelu VGG w wybranej wersji"
      ],
      "metadata": {
        "id": "Cz-BoDtdwHA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = None\n",
        "\n",
        "if image_model_type == 'vgg_16':\n",
        "  vgg = models.vgg16_bn()\n",
        "  vgg.load_state_dict(torch.load(\"/content/vgg16_bn.pth\")) \n",
        "  print(\"Used model: VGG16\")\n",
        "elif image_model_type == 'vgg_19':\n",
        "  vgg = models.vgg19_bn()\n",
        "  print(\"Used model: VGG19\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiM8HLi7umZR",
        "outputId": "9b9e9136-3756-432c-849d-22d3d6a34706"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used model: VGG19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Określenie klasy _custom_vgg_, która pozwala odtworzyć architekturę baseline ze \n",
        "źródła: w porównaniu z bazowym VGG zmieniony jest klasyfikator, po wyciągnięciu\n",
        "feature'ów przeprowadzany jest pooling, nastepnie wynik przekazywany jest do warstwy fully connected i poddawany funkcji sigmoid - wynikiem jest wektor czteroelementowy."
      ],
      "metadata": {
        "id": "ZhYCy1ApwPgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if use_trained_model==False:\n",
        "\n",
        "  # Class with own modification of VGG16 architecture - classifier was changed - \n",
        "  # after getting features from image there is pooling layer, next results are flattened and feed to fully connected layer with output number = 4\n",
        "  # at the end sigmoid function is used\n",
        "\n",
        "class custom_vgg(nn.Module):\n",
        "    \"\"\"\n",
        "        Custom vgg implementation as described in baseline in in Memotion Analisys \n",
        "        task summary (https://arxiv.org/pdf/2008.03781.pdf)\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                model: base VGG model downloaded from source\n",
        "        \"\"\"\n",
        "        super(custom_vgg, self).__init__()\n",
        "\n",
        "        self.features = list(model.features)\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "        self.pooling = model.avgpool\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(in_features=25088, out_features=4, bias=True)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                x: object to perform learning on\n",
        "        \"\"\"\n",
        "        out = self.features(x)\n",
        "        out = self.pooling(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigm(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "7-AWBscoun7-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjalizacja modelu _custom_vgg_ na wgranym wcześniej modelu VGG"
      ],
      "metadata": {
        "id": "tbQHvVYC68Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading custiom VGG16 from loaded, pretrained VGG16 model\n",
        "vgg = custom_vgg(vgg)\n",
        "\n",
        "# if use_trained_model==True:\n",
        "#   vgg16.load_state_dict(torch.load('/content/drive/MyDrive/GSN_dataset/memotion_images_model.pt'))"
      ],
      "metadata": {
        "id": "RcNr3Wd6uph2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przełączenie na korzystanie z GPU jeżeli to możliwe oraz określenie krytrium, optimizera i learning rate schedulera dla pętli uczenia"
      ],
      "metadata": {
        "id": "JcMhFeEj7FSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set VGG to run on GPU\n",
        "if use_gpu:\n",
        "    vgg.cuda()\n",
        "    \n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define optimizer and LR Scheduler for training\n",
        "optimizer_ft = optim.SGD(vgg.parameters(), lr=0.001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "IW9Ss-wxuu90"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja słownika danych dla obliczania makro F1 wraz z funkcjami operującymi na słowniku"
      ],
      "metadata": {
        "id": "0GObuiBT7S9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 score calculating for image\n",
        "image_f1_data = {\n",
        "    0: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    1: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    2: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    3: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    }\n",
        "}\n",
        "\n",
        "def clear_image_f1_data():\n",
        "    \"\"\"\n",
        "        Clearnig realted F1 data dict\n",
        "    \"\"\"\n",
        "    for i in range(4):\n",
        "        image_f1_data[i][\"true_positive\"] = 0.0\n",
        "        image_f1_data[i][\"false_positive\"] = 0.0\n",
        "        image_f1_data[i][\"false_negative\"] = 0.0\n",
        "\n",
        "def update_image_f1_data(preditions: list, labels: list):\n",
        "    \"\"\"\n",
        "        Updating F1 data dict with list of predicted class and actual labels\n",
        "        Args:\n",
        "            predictions (list): list of predicted classes with data in range 0-3\n",
        "            labels (list): list of actual classes with data in range 0-3\n",
        "    \"\"\"\n",
        "    for i in range(len(preditions)):\n",
        "        if(preditions[i] == labels[i]):\n",
        "            image_f1_data[labels[i]][\"true_positive\"] += 1.0\n",
        "        else:\n",
        "            image_f1_data[labels[i]][\"false_positive\"] += 1.0\n",
        "            image_f1_data[preditions[i]][\"false_negative\"] += 1.0\n",
        "                \n",
        "\n",
        "def calculate_image_f1_for_class(class_number: int):\n",
        "    \"\"\"\n",
        "        Calculating F1 for given class number based on data in F1 data dict\n",
        "        Args:\n",
        "            class_number(int): Class number of which F1 will be calculated\n",
        "    \"\"\"\n",
        "    precision_divider = image_f1_data[class_number][\"true_positive\"]+image_f1_data[class_number][\"false_positive\"]\n",
        "    precision = (image_f1_data[class_number][\"true_positive\"] / precision_divider) if precision_divider > 0 else 0\n",
        "    recall_divider = image_f1_data[class_number][\"true_positive\"]+image_f1_data[class_number][\"false_negative\"]\n",
        "    recall = (image_f1_data[class_number][\"true_positive\"] / recall_divider) if recall_divider > 0 else 0\n",
        "    #print(f\"Precision: {precision} , Recall: {recall} \")\n",
        "    return (2 * (precision * recall) / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "def calculate_image_macro_f1():\n",
        "    \"\"\"\n",
        "        Calculates macro F1 based on F1 score of each class - based on `calculate_image_f1_for_class`\n",
        "        function\n",
        "    \"\"\"\n",
        "    macro_f1 = 0.0\n",
        "    for i in range(4):\n",
        "        macro_f1 += calculate_image_f1_for_class(i)\n",
        "    return macro_f1/4"
      ],
      "metadata": {
        "id": "kU4rv8uUXgPw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja funkcji pomocniczej konwertującej klasy jako indeksy w macierzy do listy z indeksami na których występują dane, przykład:\n",
        "```\n",
        "[[1, 0, 0, 0],\n",
        " [0, 0, 1, 0],\n",
        " [0, 1, 0, 0],  \n",
        " [0, 0, 0, 1]]  = [0, 2, 1 ,3]\n",
        "```\n",
        "Funckja potrzebna jest przy uczeniu"
      ],
      "metadata": {
        "id": "uZcXB0Dc832l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classes_convert(classes):\n",
        "    \"\"\"\n",
        "        Converts matrix of indices (classes) to list with indices of value ex:\n",
        "        [[1, 0, 0, 0],\n",
        "         [0, 0, 1, 0],\n",
        "         [0, 1, 0, 0],  \n",
        "         [0, 0, 0, 1]]  = [0, 2, 1 ,3]\n",
        "        Args:\n",
        "            classes (list): list of lists for classes which will be converted\n",
        "    \"\"\"\n",
        "    clas = []\n",
        "    for element in range(len(classes)):\n",
        "        var = classes[element]\n",
        "        for index in range(len(var)):\n",
        "            if var[index]==1:\n",
        "                clas.append(index)\n",
        "    return torch.tensor(clas)"
      ],
      "metadata": {
        "id": "vEeAe-_azz_h"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funckja służąca do wczytania słownika stanów z pliku zewnętrznego, używana przy _transfer_learning_ oraz użyciu gotowych modeli"
      ],
      "metadata": {
        "id": "WoGPogCPBl-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, filename):\n",
        "    \"\"\"\n",
        "        Load pretrained data to model and optimizer from file\n",
        "        Args:\n",
        "            model (nn.Model): model object to which data will be loaded\n",
        "            optimizer (torch.optim): optimizer obcject to which data will be loaded\n",
        "            filename (string): file from which data will be loaded\n",
        "    \"\"\"\n",
        "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
        "    if os.path.isfile(filename):\n",
        "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "        checkpoint = torch.load(filename)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"=> loaded checkpoint '{}'\"\n",
        "                  .format(filename))\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
        "\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "E4mcdJkWeiiR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja funkcji uczącej dla gałęzi obrazu dla VGG"
      ],
      "metadata": {
        "id": "anis2HKgGlJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining training model\n",
        "def train_model(vgg, criterion, optimizer, scheduler, num_epochs=1, debug=False):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            vgg (nn.Model): Neural Network model to traing\n",
        "            criterion (nn.LossFunction): Loss Function \n",
        "            optimizer (torch.optim): Optimalization Function\n",
        "            scheduler (torch.optim.lr_scheduler): Learning Rate Scheduler\n",
        "            num_epochs (int): Number of training epochs\n",
        "            debug (boolean): Debug mode toogle\n",
        "    \"\"\"\n",
        "\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "    best_acc = 0.0\n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    avg_loss_val = 0\n",
        "    avg_acc_val = 0\n",
        "    \n",
        "    train_batches = len(dataloader)\n",
        "    val_batches = len(dataloader)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        loss_train = 0\n",
        "        loss_val = 0\n",
        "        acc_train = 0\n",
        "        acc_val = 0\n",
        "        target_true = 0\n",
        "        predicted_true = 0\n",
        "        correct_true = 0\n",
        "        \n",
        "        vgg.train(True)\n",
        "        clear_image_f1_data()\n",
        "        for inputs, classes in iter(dataloader):\n",
        "            if use_gpu:\n",
        "                sample, clas = Variable(inputs.cuda()), Variable(classes.cuda())\n",
        "            else:\n",
        "                sample, clas = Variable(inputs), Variable(classes)\n",
        "            \n",
        "            # addressing batch labels to list\n",
        "            batch_labels = []\n",
        "            for row in clas.data:\n",
        "              for i in range(len(row)):\n",
        "                if row[i] == 1:\n",
        "                  batch_labels.append(i)\n",
        "            \n",
        "            if use_gpu:\n",
        "              batch_labels = torch.tensor(batch_labels).cuda()\n",
        "            else:\n",
        "              batch_labels = torch.tensor(batch_labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = vgg(sample)\n",
        "\n",
        "            if debug==True:\n",
        "              print(outputs)\n",
        "              print(clas)\n",
        "\n",
        "            _, preds = torch.max(outputs.data, -1)\n",
        "            loss = criterion(outputs, clas)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            update_image_f1_data(preds.squeeze().tolist(), batch_labels.squeeze().tolist())\n",
        "            \n",
        "            del sample, clas, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        epoch_train_macro_f1 = calculate_image_macro_f1()\n",
        "        \n",
        "        vgg.train(False)\n",
        "        vgg.eval()\n",
        "        clear_image_f1_data()    \n",
        "        for inputs, classes in iter(dataloader):\n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda()), Variable(classes.cuda())\n",
        "            else:\n",
        "                inputs, labels = Variable(inputs), Variable(classes)\n",
        "\n",
        "            # addressing batch labels to list\n",
        "            batch_labels = []\n",
        "            for row in labels.data:\n",
        "              for i in range(len(row)):\n",
        "                if row[i] == 1:\n",
        "                  batch_labels.append(i)\n",
        "            \n",
        "            if use_gpu:\n",
        "              batch_labels = torch.tensor(batch_labels).cuda()\n",
        "            else:\n",
        "              batch_labels = torch.tensor(batch_labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = vgg(inputs)\n",
        "            \n",
        "            # Prediction\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            if debug==True:\n",
        "              print(\"Preds: \",preds)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if debug==True:\n",
        "              print(\"[1]Classes shape: \",classes.shape)\n",
        "              print(\"[1]Classes: \",classes)\n",
        "\n",
        "            classes = classes_convert(classes)\n",
        "            if debug==True:\n",
        "              print(\"[2]Classes: \",classes.shape)\n",
        "              print(\"[2]Classes: \",classes)\n",
        "            \n",
        "            update_image_f1_data(preds.squeeze().tolist(), batch_labels.squeeze().tolist())\n",
        "            del inputs, labels, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        epoch_val_macro_f1 = calculate_image_macro_f1()\n",
        "\n",
        "        print()\n",
        "        print(\"Epoch {} result: \".format(epoch))\n",
        "        print(\"F1 score: {:.4f} (train), {:.4f} (val)\".format(epoch_train_macro_f1, epoch_val_macro_f1))\n",
        "        print('-' * 10)\n",
        "        print()\n",
        "        \n",
        "        if avg_acc_val > best_acc:\n",
        "            best_acc = avg_acc_val\n",
        "            best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "        \n",
        "    elapsed_time = time.time() - since\n",
        "    print()\n",
        "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
        "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
        "    \n",
        "    vgg.load_state_dict(best_model_wts)\n",
        "    return vgg"
      ],
      "metadata": {
        "id": "d_glLctDu338"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uruchomienie uczenia w odpowiedniej wersji zgodnej z określonymi na początku założeniami tj. trenowanie od początku albo _transfer learning_ oraz zapisywanie modelu, gdy zostało to określone"
      ],
      "metadata": {
        "id": "ZMixVzzhLBAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_trained_model==False and image_model_continue_training == False:\n",
        "  vgg = train_model(vgg, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=image_epochs, debug=False)\n",
        "  if save_model:\n",
        "    torch.save(vgg.state_dict(), image_save_model_path)\n",
        "    print(\"Image model saved\")\n",
        "    checkpoint = {'state_dict': vgg.state_dict(), 'optimizer': optimizer_ft.state_dict()}\n",
        "    torch.save(checkpoint, image_checkpoint_path)\n",
        "    print(\"Checkpoint saved\")\n",
        "\n",
        "elif use_trained_model==False and image_model_continue_training == True:\n",
        "  \n",
        "  checkpoint_model, optimizer = load_checkpoint(vgg, optimizer_ft,image_checkpoint_path)\n",
        "  checkpoint_model = checkpoint_model.cuda()\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "  for state in optimizer.state.values():\n",
        "    for k,v in state.items():\n",
        "      if isinstance(v, torch.Tensor):\n",
        "        state[k] = v.cuda()\n",
        "        \n",
        "  vgg = train_model(checkpoint_model, criterion, optimizer, lr_scheduler, num_epochs=image_epochs, debug=False)\n",
        "  if save_model:\n",
        "    torch.save(vgg.state_dict(), image_save_model_path)\n",
        "    print(\"Image model saved\")\n",
        "    checkpoint = {'state_dict': vgg.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    torch.save(checkpoint, image_checkpoint_path)\n",
        "    print(\"Checkpoint saved\")\n",
        "    "
      ],
      "metadata": {
        "id": "8V6CUYAQu6A6",
        "outputId": "99337462-8862-46b7-c37b-836ad50adf7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 result: \n",
            "F1 score: 0.2467 (train), 0.1386 (val)\n",
            "----------\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 result: \n",
            "F1 score: 0.2538 (train), 0.1422 (val)\n",
            "----------\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 result: \n",
            "F1 score: 0.2602 (train), 0.1651 (val)\n",
            "----------\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 result: \n",
            "F1 score: 0.2631 (train), 0.1306 (val)\n",
            "----------\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 result: \n",
            "F1 score: 0.2827 (train), 0.1632 (val)\n",
            "----------\n",
            "\n",
            "\n",
            "Training completed in 19m 27s\n",
            "Best acc: 0.0000\n",
            "Image model saved\n",
            "Checkpoint saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gałąź przekształcania tekstu"
      ],
      "metadata": {
        "id": "63L0we3L_NCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dla przetwarzania tekstu możliwe jest użycie dwóch klasyfikatorów: (Bi)LSTM lub BERT"
      ],
      "metadata": {
        "id": "7e_LFAiuNB9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utworzenie zbioru danych dla tekstu na podstawie zbioru z Kaggle"
      ],
      "metadata": {
        "id": "U6g8zLdpXqGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utworzenie obiektów Field na podstawie tekstu wyciągnietego ze Dataset'u określanego w gałęzi obrazu. Tekst poddawany jest embeddingowi z użyciem GloVe 100d."
      ],
      "metadata": {
        "id": "ltoji6Y2QN88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "\n",
        "meme_text = dataset.data_info.iloc[:, 3]\n",
        "\n",
        "raw_df = []\n",
        "\n",
        "for i in range(len(meme_text)):\n",
        "    raw_df.append([str(meme_text[i]), dataset.labels[i]])\n",
        "\n",
        "df = pd.DataFrame(raw_df[:-3], columns=['text', 'label'])\n",
        "\n",
        "text_field = Field(\n",
        "    sequential=True,\n",
        "    tokenize='basic_english', \n",
        "    fix_length=64,\n",
        "    lower=True\n",
        ")\n",
        "label_field = Field(sequential=False, use_vocab=False)\n",
        "# prepocess\n",
        "preprocessed_text = df['text'].apply(\n",
        "    lambda x: text_field.preprocess(x)\n",
        ")\n",
        "# load fastext simple embedding with 100d\n",
        "text_field.build_vocab(\n",
        "    preprocessed_text, \n",
        "    vectors='glove.6B.100d'\n",
        ")"
      ],
      "metadata": {
        "id": "Rptz647rV-0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3116cd61-0a32-483e-af2e-a0dc7c5c8988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:14<00:00, 27029.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utworzenie zbiorów danych - uczącego i walidacyjnego - na podstawie wgranych DataFrame z biblioteki `pandas`"
      ],
      "metadata": {
        "id": "aRKErMI4SUx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataFrameDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset created drom pandas DataFrame with text data from Kaggle for \n",
        "        Memotion Analisys task\n",
        "    \"\"\"\n",
        "    def __init__(self, df: pd.DataFrame, fields: list):\n",
        "        super(DataFrameDataset, self).__init__(\n",
        "            [\n",
        "                Example.fromlist(list(r), fields) for i, r in df.iterrows()\n",
        "            ], \n",
        "            fields\n",
        "        )\n",
        "\n",
        "train_dataset, test_dataset = DataFrameDataset(\n",
        "    df=df, \n",
        "    fields=(\n",
        "        ('text', text_field),\n",
        "        ('label', label_field)\n",
        "    )\n",
        ").split(split_ratio=0.85)\n",
        "\n",
        "b_size = same_batch_size if use_trained_model else text_batch_size\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(\n",
        "    datasets=(train_dataset, test_dataset), \n",
        "    batch_sizes=(b_size, b_size),\n",
        "    sort=False\n",
        ")"
      ],
      "metadata": {
        "id": "P6qhrLcvWeyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja słownika danych dla obliczania makro F1 wraz z funkcjami operującymi na słowniku"
      ],
      "metadata": {
        "id": "nIZWmSOFZbFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_f1_data = {\n",
        "    0: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    1: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    2: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    },\n",
        "    3: {\n",
        "        \"true_positive\": 0.0,\n",
        "        \"false_positive\": 0.0,\n",
        "        \"false_negative\": 0.0\n",
        "    }\n",
        "}\n",
        "\n",
        "def clear_text_f1_data():\n",
        "    \"\"\"\n",
        "        Clearnig realted F1 data dict\n",
        "    \"\"\"\n",
        "    for i in range(4):\n",
        "        text_f1_data[i][\"true_positive\"] = 0.0\n",
        "        text_f1_data[i][\"false_positive\"] = 0.0\n",
        "        text_f1_data[i][\"false_negative\"] = 0.0\n",
        "\n",
        "def update_text_f1_data(preditions: list, labels: list):\n",
        "    \"\"\"\n",
        "        Updating F1 data dict with list of predicted class and actual labels\n",
        "        Args:\n",
        "            predictions (list): list of predicted classes with data in range 0-3\n",
        "            labels (list): list of actual classes with data in range 0-3\n",
        "    \"\"\"\n",
        "    if type(preditions) is int:\n",
        "      preditions = [preditions]\n",
        "    if type(labels) is int:\n",
        "      labels = [labels]\n",
        "    for i in range(min(len(preditions), len(labels))):\n",
        "        if(preditions[i] == labels[i]):\n",
        "            text_f1_data[labels[i]][\"true_positive\"] += 1.0\n",
        "        else:\n",
        "            text_f1_data[labels[i]][\"false_positive\"] += 1.0\n",
        "            text_f1_data[preditions[i]][\"false_negative\"] += 1.0\n",
        "                \n",
        "\n",
        "def calculate_text_f1_for_class(class_number: int):\n",
        "    \"\"\"\n",
        "        Calculating F1 for given class number based on data in F1 data dict\n",
        "        Args:\n",
        "            class_number(int): Class number of which F1 will be calculated\n",
        "    \"\"\"\n",
        "    if text_f1_data[class_number][\"true_positive\"] == 0:\n",
        "      return 0\n",
        "    precision = text_f1_data[class_number][\"true_positive\"] / (text_f1_data[class_number][\"true_positive\"]+text_f1_data[class_number][\"false_positive\"])\n",
        "    recall = text_f1_data[class_number][\"true_positive\"] / (text_f1_data[class_number][\"true_positive\"]+text_f1_data[class_number][\"false_negative\"])\n",
        "    # print(f\"Precision: {precision} , Recall: {recall} \")\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def calculate_text_macro_f1():\n",
        "     \"\"\"\n",
        "        Calculates macro F1 based on F1 score of each class - based on `calculate_image_f1_for_class`\n",
        "        function\n",
        "    \"\"\"\n",
        "    macro_f1 = 0.0\n",
        "    for i in range(4):\n",
        "        macro_f1 += calculate_text_f1_for_class(i)\n",
        "    return macro_f1/4"
      ],
      "metadata": {
        "id": "qgTTp_11Xc7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funkcja pomocnicza mająca na celu poprawne zapisanie klas poprzez znalezienie maksymalnego prawdopodobieństwa zwróconego przez klasyfikator. Używana przy uczeniu tekstu w klasyfikatorze LSTM."
      ],
      "metadata": {
        "id": "VYPGtVTRet7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_classes_convert(classes):\n",
        "    \"\"\"\n",
        "        Maps output max from probabilities to binary classes, ex.\n",
        "        [0.6, 0.4, 0.3, 0.3] = [1, 0, 0, 0]\n",
        "    \"\"\"\n",
        "    y = classes\n",
        "    # Actual conversion using y elements as index \n",
        "    M = np.zeros(len(y))\n",
        "\n",
        "    for i in range(len(y)):\n",
        "        M[i]=torch.argmax(y[i])\n",
        "    return torch.tensor(M)"
      ],
      "metadata": {
        "id": "8HoqPaoUYVj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "RM1e5Ww0_cNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ze względu na odmienne podejście w klasyfikatorze BERT zbiór danych dla niego tworzony jest osobno, również na podstawie DataField z biblioteki `pandas`"
      ],
      "metadata": {
        "id": "veb9isFChjzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset for BERT classificator with text data from Kaggle for \n",
        "        Memotion Analisys task\n",
        "    \"\"\"\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "            df - pandas DataFrame object \n",
        "        \"\"\"\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "        self.labels = [np.argmax(l) for l in df['label']]\n",
        "        self.data = [self.tokenizer(t, \n",
        "                            padding=\"max_length\", max_length=128,\n",
        "                            truncation=True, return_tensors=\"pt\") for t in df['text']]\n",
        "\n",
        "    def classes(self):\n",
        "        \"\"\"\n",
        "            Get all the classes for classifier\n",
        "        \"\"\"\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "            Get all the classes len for classifier\n",
        "        \"\"\"\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        \"\"\"\n",
        "            Fetch a batch of labels\n",
        "        \"\"\"\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        \"\"\"\n",
        "            Fetch a batch of inputs\n",
        "        \"\"\"\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "            Fetch a batch of inputs and correspoding labels\n",
        "            Returns:\n",
        "                Tuple of batch of text and batch of correspoding labels\n",
        "        \"\"\"\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "TeX0OrYcc7tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjacja zbiorów testowego, walidacyjnego i uczącego, sprawdzenie długości zbiorów"
      ],
      "metadata": {
        "id": "6AOxVAnXkPrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
        "                                     [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "print(len(df_train),len(df_val), len(df_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey4v9TE_dczE",
        "outputId": "bef906f6-eb6f-4373-a40d-1ceee0db1c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5585 698 699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja klasyfikatora BERT"
      ],
      "metadata": {
        "id": "3EgZMd0elI_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "        BERT classifier baser on PyTorch BERT clasifier, using pretrained\n",
        "        data\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.5):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 4)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "        \"\"\"\n",
        "            Perform train on given input\n",
        "            Args:\n",
        "                input_id (int) - input id on which learning should be performed\n",
        "                mask - attention mask which will be used in training\n",
        "            Returns:\n",
        "                tensor of predicted classes\n",
        "        \"\"\"\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "1Ik7CuJfeIu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja funkcji uczenia i evaluacji modelu BERT"
      ],
      "metadata": {
        "id": "VZgas9Raql7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text model train - BERT\n",
        "\n",
        "def train_bert(model, train_data, val_data, lr, epochs):\n",
        "    \"\"\"\n",
        "        Training BERT model and saving if global config is set to\n",
        "        Args:\n",
        "            model (nn.Model) - BertModel to train\n",
        "            train_data (Dataset) - dataset with data to train\n",
        "            val_Set (Dataset) - dataset to evaluate\n",
        "            lr (float) - learning rate\n",
        "            epochs (int) - number of epochs\n",
        "    \"\"\"\n",
        "    train, val = BertDataset(train_data), BertDataset(val_data)\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr = lr)\n",
        "    \n",
        "    if use_cuda:\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "            clear_text_f1_data()\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "                batch_loss = criterion(output, train_label)\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "                update_text_f1_data(output.argmax(dim=1).tolist(), train_label.squeeze().tolist())\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "            epoch_train_macro_f1 = calculate_text_macro_f1()\n",
        "            clear_text_f1_data()\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label)\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    update_text_f1_data(output.argmax(dim=1).tolist(), val_label.squeeze().tolist())\n",
        "                    total_acc_val += acc\n",
        "            epoch_val_macro_f1 = calculate_text_macro_f1()\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f} \\\n",
        "                | Train macro F1: {epoch_train_macro_f1} \\\n",
        "                | Val macro F1: {epoch_val_macro_f1} \\\n",
        "            ')\n",
        "            if save_model:\n",
        "                torch.save(model.state_dict(), text_save_model_path)\n",
        "                print(\"Image model saved\")\n",
        "                checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "                torch.save(checkpoint, text_checkpoint_path)\n",
        "                print(\"Checkpoint saved\")\n",
        "\n",
        "def evaluate_bert(model, test_data):\n",
        "    \"\"\"\n",
        "        Evaluating BERT model on given set\n",
        "        Args:\n",
        "            model (nn.Model) - BERT model\n",
        "            test_dataset (Dataset) - dataset to evaluate BERT on\n",
        "    \"\"\"\n",
        "\n",
        "    test = BertDataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    clear_text_f1_data()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "\n",
        "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "              total_acc_test += acc\n",
        "              update_text_f1_data(output.argmax(dim=1).tolist(), test_label.squeeze().tolist())\n",
        "    \n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "    print(\"F1 score: {:.4f}\".format(calculate_text_macro_f1()))"
      ],
      "metadata": {
        "id": "xhc7s-Xid2Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uruchomienie trenowania modelu BERT"
      ],
      "metadata": {
        "id": "SUFJH5PHsvdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BertClassifier()\n",
        "LR = 1e-6\n",
        "              \n",
        "train_bert(bert_model, df_train, df_val, LR, text_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615,
          "referenced_widgets": [
            "ded0605b90cb490fa695015fed4c81a4",
            "63e5962907ed4ec395edc58a6e3ac3eb",
            "d2848f05fb494988b57f84fb6360dc46",
            "bde4d6ffbcf942a2987e0e4d885af263",
            "456fb4cc92cf4b0f8240770d814ed100",
            "5f1db7fb69824015959d2ee71ffe5b25",
            "91181203f8344fcd9dcf611794e89d94",
            "9968901257494eae87a5f868ad53f01b",
            "25db280dc6414db89a3df20df9229fe6",
            "5976646f909d499f9799d85744b9d153",
            "ed28b58edb534826883a735348fa1b43",
            "277249cef9a64fa18c1adc73cd98985f",
            "6bf1f7b018b549148731e80c4697c796",
            "516bd13dd963475fbd3a4d0677765dc9",
            "1a94ece28ecc4e139deb441e8a89dcb6",
            "bd7fe67cdc334b9ca65a902e1ce94a3f",
            "6fc9aa1c55114b6b9f74732f6c34d661",
            "8f9da55fc9bf4052bb3e4e8dfa412944",
            "bc98adaac7d1423eb05a64ae3a8ddb56",
            "9bd034f005ca4f289fa8c53fed9f2c3f",
            "cd62ac6962df47dfaaec7bedac931673",
            "d69323ab281f4cacb93913a8e1f05a74",
            "dcf5022fa1644638a76b06626dd24344",
            "75ee056dbf3b4c2e8cc4323d2f0ceebb",
            "60ff5baddb9c469e903e01025ca67d23",
            "32d2451aaeb149a1afd8b9bf546543f0",
            "b3d854d137f244b4a47c1c04c53ee076",
            "c1f9682c49cf48368b6f9639a9e8883f",
            "d5f50e4a2e9144e5b55ec58f0024e0ad",
            "a6ead3bb023e42a0b0a0a53357850092",
            "ffd0eede358f4410a77f63f00649ed69",
            "797d83249c694a4291479b9ef3ee12f7",
            "f3f985b57899449b8dfa1c921bf30c54",
            "cf1d08cca3364925beb63b48502bd84b",
            "44a45dd69fd041aa93baf37d28081308",
            "09789d8c27924d358e9c89b1950b5817",
            "f68358c36d474f4ab9e1230e6ac09353",
            "ee39cf7dd28f41f381f2647dfb1cf264",
            "4dfef9ab91554547b856ade617af1bbf",
            "61b0ad20a7774ed4b91ed164ac305522",
            "331a44d92aa14d1394b72c0ab3b0fec3",
            "eca16d1472d149b295f64b470edc2b5e",
            "0eb8aad7244f450896e7e8054a70d73a",
            "eb73f69ad4664c16b6c1246fe5171c91",
            "8bdbfc32229040d9a37835fcda313058",
            "ae6e9e65783541fda60beae7a77f5b6e",
            "e34cbd6fc3af417bab975cdc8b025f7e",
            "6f300cf192ea45c388dfcb616fee9513",
            "b3ff0e13fec04dc29c467a96db3cc615",
            "c953ff44847848e39436b0f7b80dddea",
            "864f55b6d484451bb284d41a60c8a657",
            "082801b543f343f5a20b79a5a9e62e65",
            "cb1a4a5680f14e5b8fef1aba59dcc398",
            "db8cd737924b4296a5f22bf8df0c87a8",
            "f0e34374d9414286893e18ef8cdb1d37"
          ]
        },
        "id": "kZx8aIJBfMYQ",
        "outputId": "6fa8159b-d22f-4654-f51b-8d3c7deb1e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ded0605b90cb490fa695015fed4c81a4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "277249cef9a64fa18c1adc73cd98985f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcf5022fa1644638a76b06626dd24344",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf1d08cca3364925beb63b48502bd84b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bdbfc32229040d9a37835fcda313058",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 126/2793 [00:10<03:33, 12.51it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-594d8bc86301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-f64e2c61025c>\u001b[0m in \u001b[0;36mtrain_bert\u001b[0;34m(model, train_data, val_data, lr, epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mupdate_text_f1_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ewaluacja modelu BERT"
      ],
      "metadata": {
        "id": "z5Z9-qPo0yEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bert(bert_model, df_test)"
      ],
      "metadata": {
        "id": "dpEshY9vmvHJ",
        "outputId": "ad857e3f-21e9-4ef4-90a8-4f8a9f3756f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.280\n",
            "F1 score: 0.2279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "5ycjOcf8_jMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Określenie modelu LSTM"
      ],
      "metadata": {
        "id": "fUpdCsvR01lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if use_trained_model==False:\n",
        "\n",
        "class ModelParam(object):\n",
        "    \"\"\"\n",
        "        Class with parameters for TextModel\n",
        "    \"\"\"\n",
        "    def __init__(self, param_dict: dict = dict()):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                param_dict - dict with data for TextModel, possible fields:\n",
        "                    - input_size (batch size),\n",
        "                    - vocab_size (size of vocabulary),\n",
        "                    - embedding_size (same as in embedding),\n",
        "                    - target_dim (final prediction size)\n",
        "        \"\"\"\n",
        "        self.input_size = param_dict.get('input_size', 0)\n",
        "        self.vocab_size = param_dict.get('vocab_size')\n",
        "        self.embedding_dim = param_dict.get('embedding_dim', 100)\n",
        "        self.target_dim = param_dict.get('target_dim')\n",
        "        \n",
        "class TextModel(nn.Module):\n",
        "    \"\"\"\n",
        "        Class with LSTM model as described in baseline in in Memotion Analisys \n",
        "        task summary (https://arxiv.org/pdf/2008.03781.pdf)\n",
        "    \"\"\"\n",
        "    def __init__(self, model_param: ModelParam):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                model_param (ModelParam) - parameters for model in ModelParam class wrapper\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(model_param.vocab_size, model_param.embedding_dim)\n",
        "        self.conv = nn.Conv1d(64, 100, 4)\n",
        "        self.max_pool = nn.MaxPool1d(2)\n",
        "        self.lstm = nn.LSTM(48, 16, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=3200, out_features=4, bias=True)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.flatt = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            Perform training on element x\n",
        "            Args:\n",
        "                x - elem to train on\n",
        "            Returns:\n",
        "                tensor of predictions\n",
        "        \"\"\"\n",
        "        features = self.embedding(x)\n",
        "        features = F.relu(features)\n",
        "        features = self.conv(features)\n",
        "        features = self.max_pool(features)\n",
        "        features, hidden = self.lstm(features)\n",
        "        features = self.flatt(features)\n",
        "        features = self.fc(features)\n",
        "        features = self.sigm(features)\n",
        "        return features"
      ],
      "metadata": {
        "id": "BFVgxQBzX7Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trenowanie modelu LSTM"
      ],
      "metadata": {
        "id": "ca2gNerQ4OTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text model train - LSTM\n",
        "\n",
        "if use_trained_model==False:\n",
        "    model_param = ModelParam(\n",
        "    param_dict=dict(\n",
        "        vocab_size=len(text_field.vocab),\n",
        "        input_size=64,\n",
        "        embedding_dim=100,\n",
        "        target_dim=4\n",
        "        )\n",
        "    )\n",
        "    train_text_model = TextModel(model_param).cuda()\n",
        "    \n",
        "    loss_function = nn.BCELoss()\n",
        "    optimizer = optim.Adam(train_text_model.parameters(), lr=0.0002)\n",
        "    best_model_wts = copy.deepcopy(train_text_model.state_dict())\n",
        "    best_val_f1 = -1\n",
        "    epoch_count = 0\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    val_acc = []\n",
        "    for epoch in range(text_epochs):\n",
        "        train_text_model.train()\n",
        "        clear_text_f1_data()\n",
        "        b_num = 0\n",
        "        epoch_num = 0\n",
        "        epoch_losses = []\n",
        "        k = 0\n",
        "        for batch in train_iter:\n",
        "            optimizer.zero_grad()\n",
        "            prediction = train_text_model(batch.text.T.cuda())\n",
        "            batch_label = batch.label.to(torch.float)\n",
        "            labels = text_classes_convert(batch_label).cuda()\n",
        "            preds = torch.flatten(torch.max(prediction, 1)[1]).float().cuda()\n",
        "            # print(prediction.shape)\n",
        "            # print(batch.label.shape)\n",
        "            loss = loss_function(torch.squeeze(prediction), torch.squeeze(batch.label.to(torch.float)).cuda())\n",
        "            #update_text_f1_data(preds.squeeze().tolist(), labels.squeeze().tolist())\n",
        "            # print(prediction)\n",
        "            # print(batch.label)\n",
        "            # print(preds)\n",
        "            # print(labels)\n",
        "            update_text_f1_data(preds.tolist(), labels.tolist())\n",
        "            loss.backward()\n",
        "            epoch_losses.append(loss.item())\n",
        "            optimizer.step()\n",
        "            k = k+1\n",
        "        \n",
        "        losses.append(sum(epoch_losses)/k)\n",
        "        train_text_macro_f1 = calculate_text_macro_f1()\n",
        "        \n",
        "        train_text_model.eval()\n",
        "        clear_text_f1_data()\n",
        "        val_epoch_losses = []\n",
        "        k = 0\n",
        "        total_acc_val = 0\n",
        "        \n",
        "        test_iter_elements_number = 0\n",
        "        for batch in test_iter:\n",
        "            with torch.no_grad():\n",
        "                optimizer.zero_grad()\n",
        "                prediction = train_text_model(batch.text.T.cuda())\n",
        "                batch.label = batch.label.to(torch.float)\n",
        "                labels = text_classes_convert(batch.label).cuda()\n",
        "                # preds = torch.flatten(torch.max(prediction, 1)[1]).float().cuda()\n",
        "                _, preds = torch.max(prediction, 1)\n",
        "                # print(preds)\n",
        "                # print(preds2)\n",
        "                loss = loss_function(torch.squeeze(prediction), torch.squeeze(batch.label.to(torch.float)).cuda())\n",
        "                #update_text_f1_data(preds.squeeze().tolist(), labels.squeeze().tolist())\n",
        "                # print(prediction)\n",
        "                # print(batch.label)\n",
        "                # print(preds)\n",
        "                # print(labels)\n",
        "                \n",
        "                test_iter_elements_number += len(labels)\n",
        "                acc = (preds == labels).sum().item()\n",
        "                # print(acc)\n",
        "                # print(loss)\n",
        "                total_acc_val += acc\n",
        "                val_epoch_losses.append(loss.item())\n",
        "                update_text_f1_data(preds.tolist(), labels.tolist())\n",
        "                k = k+1\n",
        "\n",
        "        # print(total_acc_val)\n",
        "        # print(test_iter_elements_number)\n",
        "        val_acc.append(total_acc_val / test_iter_elements_number)\n",
        "        val_losses.append(sum(val_epoch_losses)/k)\n",
        "        val_text_macro_f1 = calculate_text_macro_f1()\n",
        "        if val_text_macro_f1 > best_val_f1:\n",
        "            best_val_f1 = val_text_macro_f1\n",
        "            best_model_wts = copy.deepcopy(train_text_model.state_dict())\n",
        "        else :\n",
        "            train_text_model.load_state_dict(best_model_wts)\n",
        "        print()\n",
        "        print(\"Epoch {} result: \".format(epoch))\n",
        "        print(\"F1 score: {:.4f} (train), {:.4f} (val)\".format(train_text_macro_f1, val_text_macro_f1))\n",
        "        print('-' * 10)\n",
        "        print()\n",
        "    print(f\"Best val f1 score: {best_val_f1}\")\n",
        "    if save_model:\n",
        "        torch.save(best_model_wts, text_save_model_path)\n",
        "        print(\"Text model saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "yjVN5puCYh1G",
        "outputId": "24ae5c26-0dca-4fae-c745-53fc3c85e470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 result: \n",
            "F1 score: 0.2187 (train), 0.1287 (val)\n",
            "----------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-655f077c0c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testowanie wytrenowanych modeli"
      ],
      "metadata": {
        "id": "-KspV4xGLPv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie danych do testowania (w wypadku jednoczesnego trenowania i testowania wczytywany jest drugi raz ten sam zbiór)"
      ],
      "metadata": {
        "id": "qfEtq3Ee5NUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset with data for Memotion Analisys Kaggle for tests\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path, low_data_mode=False, debug=False):\n",
        "         \"\"\"\n",
        "            Args:\n",
        "            csv_path (string): path to csv file with data \n",
        "            low_data_mode (boolean): low data mode for testing \n",
        "            debug (boolean): enable debug options\n",
        "        \"\"\"\n",
        "        self.debug = debug\n",
        "        # Read the csv_file\n",
        "        if low_data_mode==True:\n",
        "          self.data_info = pd.read_csv(csv_path, header = 6952)\n",
        "        else:\n",
        "          self.data_info = pd.read_csv(csv_path, header = 3)\n",
        "\n",
        "        # Column containing image names\n",
        "        self.image_arr = np.asarray(self.data_info.iloc[:, 1])\n",
        "        # Columns containing emotions classification\n",
        "        self.humour_arr = np.asarray(self.data_info.iloc[:, 4])\n",
        "        self.sarcasm_arr = np.asarray(self.data_info.iloc[:, 5])\n",
        "        self.offensive_arr = np.asarray(self.data_info.iloc[:, 6])\n",
        "        self.motivational_arr = np.asarray(self.data_info.iloc[:, 7])\n",
        "        \n",
        "        # Transforms performed on loaded image\n",
        "        self.data_transforms = transforms.Compose([\n",
        "                                      transforms.Resize((224, 224)),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        \n",
        "        # Array with class vectors for each image\n",
        "        self.labels = []\n",
        "        self.text_arr = self.data_info.iloc[:, 3]\n",
        "        raw_df = []\n",
        "\n",
        "        for i in range(len(self.text_arr)):\n",
        "            raw_df.append([str(self.text_arr[i])])\n",
        "        self.df = pd.DataFrame(raw_df[:-2], columns=['text'])\n",
        "\n",
        "        self.text_field = Field(\n",
        "            sequential=True,\n",
        "            tokenize='basic_english', \n",
        "            fix_length=50, \n",
        "            lower=True\n",
        "        )\n",
        "        # prepocess\n",
        "        preprocessed_text = self.df['text'].apply(\n",
        "            lambda x: self.text_field.preprocess(x)\n",
        "        )\n",
        "        # load fastext simple embedding with 100d\n",
        "        self.text_field.build_vocab(\n",
        "            preprocessed_text, \n",
        "            vectors='glove.6B.100d'\n",
        "        )\n",
        "\n",
        "        class DataFrameDataset(Dataset):\n",
        "              \"\"\"\n",
        "                    Dataset created drom pandas DataFrame with text data from Kaggle for \n",
        "                    Memotion Analisys task\n",
        "              \"\"\"\n",
        "            def __init__(self, df: pd.DataFrame, fields: list):\n",
        "                super(DataFrameDataset, self).__init__(\n",
        "                    [\n",
        "                        Example.fromlist(list(r), fields) for i, r in df.iterrows()\n",
        "                    ], \n",
        "                    fields\n",
        "                )\n",
        "        self.text_dataset = DataFrameDataset(\n",
        "            df=df, \n",
        "            fields=(\n",
        "                ('text', text_field),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.text_iter = BucketIterator(\n",
        "            dataset=train_dataset, \n",
        "            batch_size=1\n",
        "        )\n",
        "\n",
        "        # Mapping word classification to 4 numeric classes\n",
        "        for index in range(len(self.humour_arr)):\n",
        "          humour_value = class_humour_weights[self.humour_arr[index]]\n",
        "          sarcasm_value = class_sarcasm_weights[self.sarcasm_arr[index]]\n",
        "          offensive_value = class_offensive_weights[self.offensive_arr[index]]\n",
        "          motivational_value = class_motivational_weights[self.motivational_arr[index]]\n",
        "\n",
        "          if humour_value > sarcasm_value:\n",
        "            if humour_value > offensive_value:\n",
        "              if humour_value > motivational_value:\n",
        "                var = 0\n",
        "              else:\n",
        "                var = 3 \n",
        "            else:\n",
        "              if offensive_value > motivational_value:\n",
        "                var = 2\n",
        "              else: \n",
        "                var = 3\n",
        "          else:\n",
        "            if sarcasm_value > offensive_value:\n",
        "              if sarcasm_value > motivational_value:\n",
        "                var = 1\n",
        "              else:\n",
        "                var = 3\n",
        "            else: \n",
        "              if offensive_value > motivational_value: \n",
        "                var = 2\n",
        "              else:\n",
        "                var = 3\n",
        "\n",
        "          # Creating class vector\n",
        "          lab = [0.0, 0.0, 0.0, 0.0]\n",
        "          lab[var] = 1.0\n",
        "          \n",
        "          # Adding new image class vector to labels array\n",
        "          self.labels.append(lab) \n",
        "\n",
        "        # Calculate of dataset\n",
        "        self.data_len = len(self.data_info.index)\n",
        "        \n",
        "        # Set correct path to images\n",
        "        self.image_arr = images_dir + self.image_arr\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          index (int): index of item to get  \n",
        "\n",
        "        Returns:\n",
        "          Tuple of image, text and class vector as tensors\n",
        "        \"\"\"\n",
        "        img_as_img = None\n",
        "        single_image_name = None\n",
        "\n",
        "        try:\n",
        "          # Get image name from pandas df\n",
        "          single_image_name = self.image_arr[index]\n",
        "\n",
        "          # # Open image with PIL and convert to RGB image\n",
        "          img = Image.open(single_image_name).convert('RGB')\n",
        "          if self.debug==True:\n",
        "            print('1:', img)\n",
        "\n",
        "          # Transform image and convert to tensor\n",
        "          img_as_tensor = self.data_transforms(img)\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('2:', img_as_tensor)\n",
        "\n",
        "          # Get class vector of the image from labels array\n",
        "          label = self.labels[index]\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('3:',label)\n",
        "\n",
        "          # Convert class vector to tensor\n",
        "          label = torch.as_tensor(label)\n",
        "          \n",
        "          if self.debug==True:\n",
        "            print('4:',label)\n",
        "\n",
        "          text_data = next(iter(self.text_iter)).text\n",
        "\n",
        "          if self.debug==True:\n",
        "            print('5:',text_data)\n",
        "\n",
        "          return (img_as_tensor, text_data, label)\n",
        "        except:\n",
        "          print(\"Image loading error for:\",single_image_name)\n",
        "          return ('ERROR', torch.tensor([-1]))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data_len    \n"
      ],
      "metadata": {
        "id": "ciwGsrcWkQDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjalizacja testowego zbioru danych"
      ],
      "metadata": {
        "id": "jBdtZYt67SpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TestDataset('MemotionAnalysis/labels.csv', low_data_mode=False, debug=False)\n",
        "\n",
        "# Loading dataset into DataLoader and setting batch_size\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "AR7pRxEUx2XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdzenie poprawnosci wczytania danych"
      ],
      "metadata": {
        "id": "OFphol1t7YL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, text, label = next(iter(dataloader))\n",
        "print(image.shape)\n",
        "print(text.shape)\n",
        "print(label.shape)"
      ],
      "metadata": {
        "id": "IPTM_Gd6yZeM",
        "outputId": "96eee21c-7381-441b-d1e8-ad3020ccc3f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 224, 224])\n",
            "torch.Size([4, 50, 1])\n",
            "torch.Size([4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjalizacja modeli dla tekstu i obrazu: w razie uczenia i testowania użyte są modele z treningu, w razie użycia wcześniej wyternowanych zbiorów zostają one wczytane w podanych na poczatkui pliku ścieżek"
      ],
      "metadata": {
        "id": "9X09079j7cW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_model, image_model = None, None\n",
        "if use_trained_model:\n",
        "    text_model_dict = torch.load(text_load_model_path, map_location=torch.device('cpu') if not use_gpu else None)\n",
        "    image_model_dict = torch.load(image_load_model_path, map_location=torch.device('cpu') if not use_gpu else None)\n",
        "    vgg16 = models.vgg16_bn()\n",
        "    vgg16.load_state_dict(torch.load(\"/content/vgg16_bn.pth\")) \n",
        "    image_model = custom_vgg16(vgg16)\n",
        "    model_param = ModelParam(\n",
        "    param_dict=dict(\n",
        "        vocab_size=len(text_field.vocab),\n",
        "        input_size=50,\n",
        "        embedding_dim=100,\n",
        "        target_dim=4\n",
        "        )\n",
        "    )\n",
        "    text_model = TextModel(model_param)\n",
        "    image_model.load_state_dict(image_model_dict)\n",
        "    text_model.load_state_dict(text_model_dict)\n",
        "else:\n",
        "    text_model = train_text_model\n",
        "    image_model = vgg16"
      ],
      "metadata": {
        "id": "a9_dXxxurkAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdzenie poprawności wczytania modeli"
      ],
      "metadata": {
        "id": "jZmrBKLZ7uEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_model)\n",
        "print(image_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0CymgUquPvY",
        "outputId": "fdc0715b-0863-48e4-eb03-a37f3e917295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextModel(\n",
            "  (embedding): Embedding(13676, 100)\n",
            "  (conv): Conv1d(50, 4, kernel_size=(1,), stride=(1,))\n",
            "  (max_pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (lstm): LSTM(50, 16)\n",
            "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
            "  (sigm): Sigmoid()\n",
            ")\n",
            "custom_vgg16(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (pooling): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc): Linear(in_features=25088, out_features=4, bias=True)\n",
            "  (sigm): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicja funkcji pomocniczej konwertującej klasy jako indeksy w macierzy do listy z indeksami na których występują dane, przykład:\n",
        "```\n",
        "[[1, 0, 0, 0],\n",
        " [0, 0, 1, 0],\n",
        " [0, 1, 0, 0],  \n",
        " [0, 0, 0, 1]]  = [0, 2, 1 ,3]\n",
        "```\n",
        "Funckja potrzebna jest przy ewaluacji"
      ],
      "metadata": {
        "id": "XSdKJpu18S1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels_as_indices(labels_data):\n",
        "    \"\"\"\n",
        "    Converts matrix of indices (classes) to list with indices of value ex:\n",
        "        [[1, 0, 0, 0],\n",
        "         [0, 0, 1, 0],\n",
        "         [0, 1, 0, 0],  \n",
        "         [0, 0, 0, 1]]  = [0, 2, 1 ,3]\n",
        "        Args:\n",
        "            classes (list): list of lists for classes which will be converted\n",
        "    \"\"\" \n",
        "    return torch.flatten(torch.max(labels_data, 1)[1]).float()"
      ],
      "metadata": {
        "id": "kIIs7eKhas9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdefiniowanie funkcji predykującej"
      ],
      "metadata": {
        "id": "hX0lb_Wa9ZR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(image_data, text_data):\n",
        "    \"\"\"\n",
        "        Predict class of a memes\n",
        "        Args:\n",
        "            image_data - data of images to predict\n",
        "            text_data - data of text to predict\n",
        "        Returns:\n",
        "            list of predicitions\n",
        "    \"\"\"\n",
        "    image_pred = image_model(image_data)\n",
        "    text_pred = torch.squeeze(text_model(text_data))\n",
        "    preds = image_pred*0.3 + text_pred*0.7\n",
        "    images_preds = get_labels_as_indices(image_pred)\n",
        "    text_preds = get_labels_as_indices(text_pred)\n",
        "    print(f\"Image pred: {images_preds}\")\n",
        "    print(f\"Text pred: {text_preds}\")\n",
        "    preds = get_labels_as_indices(preds)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "1dz2aOItwvhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przykładowa predykcja danych"
      ],
      "metadata": {
        "id": "5DmIj0bB9c1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image_inputs, image_classes = next(iter(dataloader))\n",
        "# text_data = next(iter(train_iter))\n",
        "\n",
        "image_data, text_data, label = next(iter(dataloader))\n",
        "\n",
        "preds = predict(image_data, torch.squeeze(text_data))\n",
        "\n",
        "print(preds)\n",
        "print(get_labels_as_indices(label))\n",
        "# print(get_labels_as_indices(text_data.label))"
      ],
      "metadata": {
        "id": "kITwhpuTxgX6",
        "outputId": "610a3d34-0372-4b41-c4a0-05417c85474e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image pred: tensor([2., 2., 2., 1.])\n",
            "Text pred: tensor([3., 3., 3., 1.])\n",
            "tensor([2., 2., 3., 1.])\n",
            "tensor([2., 1., 0., 3.])\n"
          ]
        }
      ]
    }
  ]
}